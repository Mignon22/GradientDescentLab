{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac4fd95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from gdlib.Ex1_42_adam_SGD import Ex1_42_adam_SGD\n",
    "from gdlib.Ex1_43_adam_SGDR import Ex1_43_adam_SGDR\n",
    "from gdlib.Ex1_44_adam_minibatch import Ex1_44_adam_MB\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475d53fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_and_lr(loss_history_train, loss_history_test, lr_history = None, min_index = None, min_test_loss = None, title_main = 'Experiment : Loss vs Learning Rate', subtitle = '', figsize = (8, 5.5), save_path = None):\n",
    "    loss_history_train = np.array(loss_history_train)\n",
    "    loss_history_test = np.array(loss_history_test)\n",
    "\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "    fig, ax1 = plt.subplots(figsize = figsize)\n",
    "    ax1.set_title(title_main, fontsize = 14, fontweight = 'bold', pad = 24)\n",
    "    if subtitle:\n",
    "        fig.text(0.5, 0.893, subtitle, ha = 'center', fontsize = 12, style = 'italic')\n",
    "    \n",
    "    # 主軸\n",
    "    color1 = '#1f77bc'\n",
    "    color2 = '#ff7f0e'\n",
    "    color3 = '#003366'\n",
    "    ax1.set_xlabel('Iterations', fontsize = 14, fontweight = 'bold')\n",
    "    ax1.set_ylabel('Loss', color = color1, fontsize = 14, fontweight = 'bold')\n",
    "    ax1.plot(loss_history_train, label = 'Train Loss', color = color1, linewidth = 3, alpha = 0.85, zorder = 3)\n",
    "    ax1.plot(loss_history_test, label = 'Test Loss', color = color2, linewidth = 3, linestyle = 'dashed', alpha = 0.85, zorder = 3)\n",
    "    ax1.tick_params(axis = 'y', labelcolor = color1)\n",
    "\n",
    "    ax1.plot(min_index, min_test_loss, 'o', markersize = 5, label = 'Min Test Loss', color = color3, zorder = 4)\n",
    "\n",
    "    bbox_props = dict(boxstyle = 'round,pad = 0.4', fc = 'white', lw = 0.8, alpha = 0.85)\n",
    "    ax1.annotate(\n",
    "        f'Min : {min_test_loss:.4f} @ Iter {min_index}',\n",
    "        xy = (min_index, min_test_loss),\n",
    "        xytext = (max(min_index - len(loss_history_test) * 0.2, 0), min_test_loss + (loss_history_test.max() - loss_history_test.min()) * 0.2),\n",
    "        textcoords = 'data',\n",
    "        arrowprops = dict(arrowstyle = '-|>', color = color3, lw = 1.2, zorder = 4),\n",
    "        fontsize = 12,\n",
    "        color = color3,\n",
    "        bbox = bbox_props,\n",
    "        zorder = 4\n",
    "    )\n",
    "    ax1.legend(loc = 'upper right', fontsize = 11, framealpha = 0.9)\n",
    "    ax1.grid(True, linestyle = 'dashed', linewidth = 0.5, alpha = 0.4, zorder = 0)\n",
    "\n",
    "    # 副軸\n",
    "    if lr_history is not None:\n",
    "        lr_history = np.array(lr_history)\n",
    "        ax2 = ax1.twinx()\n",
    "        color4 = '#2ca02c'\n",
    "        ax2.set_ylabel('Learning Rate', color = color4, fontsize = 14, fontweight = 'bold')\n",
    "        ax2.plot(lr_history, label = 'Learning Rate', color = color4, linestyle = 'dotted', linewidth = 2, zorder = 1)\n",
    "        ax2.tick_params(axis = 'y', labelcolor = color4)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    if save_path is not None:\n",
    "        fig.savefig(save_path, dpi = 720, bbox_inches = 'tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4610d835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Prepared ===\n",
      "mean =  [30.1526  1.99   14.6872]\n",
      "scale =  [11.9856  1.0246  8.5246]\n",
      "Iteration 0 : w = [0.18   0.3168 1.0115], b = 0.3713, Train Loss = 2215.1138, Test Loss = 1864.3051\n",
      "Iteration 50 : w = [0.9317 0.9127 0.5695], b = 2.2065, Train Loss = 2010.5674, Test Loss = 1694.5460\n",
      "Iteration 100 : w = [1.706  1.4973 0.206 ], b = 3.9616, Train Loss = 1823.1782, Test Loss = 1537.9824\n",
      "Iteration 150 : w = [2.2235 1.9559 0.1538], b = 5.5355, Train Loss = 1674.0847, Test Loss = 1407.9048\n",
      "Iteration 200 : w = [2.7467 2.5028 0.1206], b = 7.1961, Train Loss = 1523.4573, Test Loss = 1276.4055\n",
      "Iteration 250 : w = [ 3.8068  3.1966 -0.2487], b = 8.9347, Train Loss = 1353.4358, Test Loss = 1135.8808\n",
      "Iteration 300 : w = [ 4.5883  3.8933 -0.7754], b = 10.6028, Train Loss = 1204.4121, Test Loss = 1014.8238\n",
      "Iteration 350 : w = [ 5.0306  4.4729 -0.9547], b = 12.0810, Train Loss = 1089.7394, Test Loss = 916.9896\n",
      "Iteration 400 : w = [ 5.5633  4.7936 -1.3405], b = 13.6492, Train Loss = 976.1045, Test Loss = 822.7221\n",
      "Iteration 450 : w = [ 5.8271  4.7837 -1.4363], b = 15.0177, Train Loss = 892.5758, Test Loss = 749.6643\n",
      "Iteration 500 : w = [ 6.6077  5.1379 -1.4983], b = 16.5724, Train Loss = 789.2604, Test Loss = 660.8138\n",
      "Iteration 550 : w = [ 7.3293  5.77   -1.6921], b = 18.0639, Train Loss = 692.9537, Test Loss = 579.9991\n",
      "Iteration 600 : w = [ 7.7581  6.1474 -2.0447], b = 19.4450, Train Loss = 614.0212, Test Loss = 514.9334\n",
      "Iteration 650 : w = [ 8.1314  6.2346 -2.3248], b = 20.8023, Train Loss = 544.8694, Test Loss = 456.9574\n",
      "Iteration 700 : w = [ 8.6222  6.6691 -2.4317], b = 22.0659, Train Loss = 481.5309, Test Loss = 403.1103\n",
      "Iteration 750 : w = [ 9.1054  6.8704 -2.5887], b = 23.3021, Train Loss = 424.8307, Test Loss = 355.2553\n",
      "Iteration 800 : w = [ 9.2064  6.8352 -2.6347], b = 24.4346, Train Loss = 381.1424, Test Loss = 316.8854\n",
      "Iteration 850 : w = [ 9.8198  7.2994 -2.7602], b = 25.6198, Train Loss = 331.3787, Test Loss = 275.4990\n",
      "Iteration 900 : w = [10.0094  7.4058 -3.0919], b = 26.7605, Train Loss = 289.7145, Test Loss = 241.3163\n",
      "Iteration 950 : w = [10.3428  7.6346 -3.3075], b = 27.7807, Train Loss = 254.2660, Test Loss = 212.1173\n",
      "Iteration 1000 : w = [10.5657  7.7571 -3.6619], b = 28.8062, Train Loss = 221.1883, Test Loss = 185.3726\n",
      "Iteration 1050 : w = [10.9124  7.9872 -3.7339], b = 29.7838, Train Loss = 192.9384, Test Loss = 161.5131\n",
      "Iteration 1100 : w = [11.2119  8.2624 -3.712 ], b = 30.6263, Train Loss = 171.1948, Test Loss = 142.9671\n",
      "Iteration 1150 : w = [11.3597  8.3943 -3.9406], b = 31.4473, Train Loss = 150.5754, Test Loss = 126.1770\n",
      "Iteration 1200 : w = [11.4717  8.3983 -4.1398], b = 32.2745, Train Loss = 131.3934, Test Loss = 110.3150\n",
      "Iteration 1250 : w = [11.4104  8.3179 -4.3334], b = 32.9897, Train Loss = 116.1179, Test Loss = 97.5800\n",
      "Iteration 1300 : w = [11.5172  8.2839 -4.3823], b = 33.6959, Train Loss = 102.4154, Test Loss = 85.9669\n",
      "Iteration 1350 : w = [11.6758  8.2401 -4.435 ], b = 34.3772, Train Loss = 89.9878, Test Loss = 75.5619\n",
      "Iteration 1400 : w = [11.8052  8.1983 -4.5284], b = 35.0711, Train Loss = 78.2173, Test Loss = 65.8529\n",
      "Iteration 1450 : w = [11.9532  8.0713 -4.8093], b = 35.8544, Train Loss = 65.2602, Test Loss = 55.4958\n",
      "Iteration 1500 : w = [12.0187  8.0245 -4.9089], b = 36.3958, Train Loss = 57.6038, Test Loss = 49.2707\n",
      "Iteration 1550 : w = [12.181   7.992  -5.0855], b = 36.9696, Train Loss = 49.7904, Test Loss = 43.0751\n",
      "Iteration 1600 : w = [12.1997  7.8565 -5.2392], b = 37.5118, Train Loss = 43.1315, Test Loss = 37.7401\n",
      "Iteration 1650 : w = [12.2982  7.7146 -5.5667], b = 38.0214, Train Loss = 36.8396, Test Loss = 32.9040\n",
      "Iteration 1700 : w = [12.3788  7.6255 -5.847 ], b = 38.5454, Train Loss = 31.3802, Test Loss = 28.7014\n",
      "Iteration 1750 : w = [12.5343  7.6448 -5.888 ], b = 38.9503, Train Loss = 27.9140, Test Loss = 25.9053\n",
      "Iteration 1800 : w = [12.5186  7.4584 -6.0872], b = 39.3622, Train Loss = 24.4252, Test Loss = 23.2274\n",
      "Iteration 1850 : w = [12.5742  7.3931 -6.1952], b = 39.6660, Train Loss = 22.1798, Test Loss = 21.4903\n",
      "Iteration 1900 : w = [12.6845  7.3436 -6.3459], b = 39.9161, Train Loss = 20.3246, Test Loss = 20.0610\n",
      "Iteration 1950 : w = [12.7894  7.2473 -6.4702], b = 40.1974, Train Loss = 18.4392, Test Loss = 18.5892\n",
      "Iteration 2000 : w = [12.9345  7.1876 -6.5469], b = 40.4567, Train Loss = 16.8565, Test Loss = 17.3122\n",
      "Iteration 2050 : w = [12.9755  7.0596 -6.5648], b = 40.6947, Train Loss = 15.5824, Test Loss = 16.3201\n",
      "Iteration 2100 : w = [13.0827  6.9934 -6.6004], b = 40.8940, Train Loss = 14.5492, Test Loss = 15.4885\n",
      "Iteration 2150 : w = [13.1994  6.9512 -6.701 ], b = 41.1372, Train Loss = 13.4872, Test Loss = 14.6542\n",
      "Iteration 2200 : w = [13.2848  6.8158 -6.6579], b = 41.1805, Train Loss = 13.0160, Test Loss = 14.2334\n",
      "Iteration 2250 : w = [13.3609  6.693  -6.6166], b = 41.3664, Train Loss = 12.2095, Test Loss = 13.6126\n",
      "Iteration 2300 : w = [13.4423  6.5829 -6.6404], b = 41.4739, Train Loss = 11.6562, Test Loss = 13.1664\n",
      "Iteration 2350 : w = [13.5345  6.469  -6.6642], b = 41.6547, Train Loss = 10.9612, Test Loss = 12.6374\n",
      "Iteration 2400 : w = [13.6324  6.3337 -6.656 ], b = 41.7565, Train Loss = 10.4460, Test Loss = 12.2208\n",
      "Iteration 2450 : w = [13.6993  6.2321 -6.6928], b = 41.8241, Train Loss = 10.0993, Test Loss = 11.9347\n",
      "Iteration 2500 : w = [13.8098  6.1379 -6.7639], b = 41.8080, Train Loss = 9.8427, Test Loss = 11.6385\n",
      "Iteration 2550 : w = [13.9091  6.0697 -6.7917], b = 41.9106, Train Loss = 9.4536, Test Loss = 11.3069\n",
      "Iteration 2600 : w = [13.9575  5.9969 -6.7852], b = 42.0296, Train Loss = 9.1387, Test Loss = 11.0912\n",
      "Iteration 2650 : w = [14.1358  6.0408 -6.8452], b = 42.0809, Train Loss = 8.9076, Test Loss = 10.7743\n",
      "Iteration 2700 : w = [14.2392  5.9503 -6.7324], b = 42.2459, Train Loss = 8.5014, Test Loss = 10.5104\n",
      "Iteration 2750 : w = [14.1573  5.7224 -6.7503], b = 42.2904, Train Loss = 8.3128, Test Loss = 10.4846\n",
      "Iteration 2800 : w = [14.3557  5.7828 -6.86  ], b = 42.2098, Train Loss = 8.1900, Test Loss = 10.1348\n",
      "Iteration 2850 : w = [14.4387  5.6717 -6.8694], b = 42.2770, Train Loss = 7.9084, Test Loss = 9.8976\n",
      "Iteration 2900 : w = [14.493   5.486  -6.8737], b = 42.3650, Train Loss = 7.5885, Test Loss = 9.6831\n",
      "Iteration 2950 : w = [14.697   5.4424 -6.7765], b = 42.3665, Train Loss = 7.3025, Test Loss = 9.3406\n",
      "Iteration 3000 : w = [14.7919  5.3783 -6.8215], b = 42.3389, Train Loss = 7.1587, Test Loss = 9.1350\n",
      "Iteration 3050 : w = [14.8772  5.3087 -6.81  ], b = 42.4769, Train Loss = 6.9341, Test Loss = 8.9860\n",
      "Iteration 3100 : w = [14.9447  5.0991 -6.6879], b = 42.6886, Train Loss = 6.6277, Test Loss = 8.9366\n",
      "Iteration 3150 : w = [15.0585  5.0353 -6.6913], b = 42.6733, Train Loss = 6.4576, Test Loss = 8.7124\n",
      "Iteration 3200 : w = [15.0533  4.875  -6.7655], b = 42.7049, Train Loss = 6.3298, Test Loss = 8.6304\n",
      "Iteration 3250 : w = [15.2109  4.7844 -6.822 ], b = 42.5777, Train Loss = 6.1006, Test Loss = 8.2371\n",
      "Iteration 3300 : w = [15.4211  4.8117 -6.777 ], b = 42.4817, Train Loss = 5.9833, Test Loss = 7.9518\n",
      "Iteration 3350 : w = [15.5189  4.7516 -6.8139], b = 42.6176, Train Loss = 5.8379, Test Loss = 7.8373\n",
      "Iteration 3400 : w = [15.6352  4.6616 -6.8442], b = 42.5343, Train Loss = 5.7000, Test Loss = 7.6103\n",
      "Iteration 3450 : w = [15.8194  4.6579 -6.9042], b = 42.5845, Train Loss = 5.6589, Test Loss = 7.4602\n",
      "Iteration 3500 : w = [15.9106  4.6018 -6.7965], b = 42.6317, Train Loss = 5.5635, Test Loss = 7.4101\n",
      "Iteration 3550 : w = [15.8873  4.334  -6.7547], b = 42.6777, Train Loss = 5.2418, Test Loss = 7.2638\n",
      "Iteration 3600 : w = [15.9543  4.2074 -6.8208], b = 42.5582, Train Loss = 5.1095, Test Loss = 7.0497\n",
      "Iteration 3650 : w = [16.0633  4.1654 -6.7715], b = 42.5159, Train Loss = 5.0394, Test Loss = 6.9336\n",
      "Iteration 3700 : w = [16.1027  4.0051 -6.8871], b = 42.5977, Train Loss = 4.8909, Test Loss = 6.8230\n",
      "Iteration 3750 : w = [16.2486  3.8622 -6.7742], b = 42.5862, Train Loss = 4.7251, Test Loss = 6.6714\n",
      "Iteration 3800 : w = [16.3281  3.7782 -6.7615], b = 42.5320, Train Loss = 4.6515, Test Loss = 6.5637\n",
      "Iteration 3850 : w = [16.4277  3.6869 -6.7264], b = 42.4669, Train Loss = 4.5835, Test Loss = 6.4561\n",
      "Iteration 3900 : w = [16.4825  3.5253 -6.7913], b = 42.4501, Train Loss = 4.4892, Test Loss = 6.3630\n",
      "Iteration 3950 : w = [16.5681  3.4712 -6.8297], b = 42.5316, Train Loss = 4.4007, Test Loss = 6.2765\n",
      "Iteration 4000 : w = [16.712   3.416  -6.7389], b = 42.6318, Train Loss = 4.3134, Test Loss = 6.2126\n",
      "Iteration 4050 : w = [16.7065  3.306  -6.7212], b = 42.5764, Train Loss = 4.2847, Test Loss = 6.2061\n",
      "Iteration 4100 : w = [16.7744  3.3157 -6.778 ], b = 42.3989, Train Loss = 4.3125, Test Loss = 6.0942\n",
      "Iteration 4150 : w = [16.7792  3.3468 -6.8481], b = 42.4089, Train Loss = 4.3187, Test Loss = 6.0744\n",
      "Iteration 4200 : w = [16.8172  3.2608 -6.8625], b = 42.2724, Train Loss = 4.3607, Test Loss = 6.0717\n",
      "Iteration 4250 : w = [16.8959  3.2762 -6.8157], b = 42.3754, Train Loss = 4.2776, Test Loss = 5.9941\n",
      "Iteration 4300 : w = [17.0552  3.2379 -6.6913], b = 42.4797, Train Loss = 4.2210, Test Loss = 5.9509\n",
      "Iteration 4350 : w = [16.9446  3.0875 -6.6844], b = 42.5069, Train Loss = 4.1610, Test Loss = 6.0147\n",
      "Iteration 4400 : w = [16.8868  3.0102 -6.8702], b = 42.4841, Train Loss = 4.1773, Test Loss = 6.0249\n",
      "Iteration 4450 : w = [16.8749  2.9585 -6.8349], b = 42.5150, Train Loss = 4.1789, Test Loss = 6.0704\n",
      "Iteration 4500 : w = [16.9648  2.9578 -6.8131], b = 42.5732, Train Loss = 4.1087, Test Loss = 5.9859\n",
      "Iteration 4550 : w = [17.0509  2.9891 -6.873 ], b = 42.4688, Train Loss = 4.1048, Test Loss = 5.8716\n",
      "Iteration 4600 : w = [16.9837  2.8784 -6.926 ], b = 42.4293, Train Loss = 4.1617, Test Loss = 5.9769\n",
      "Iteration 4650 : w = [16.9978  2.8139 -7.0406], b = 42.5245, Train Loss = 4.1712, Test Loss = 6.0162\n",
      "Iteration 4700 : w = [17.1342  2.8152 -7.0185], b = 42.4753, Train Loss = 4.0958, Test Loss = 5.8609\n",
      "Iteration 4750 : w = [17.2145  2.8371 -6.9591], b = 42.6866, Train Loss = 4.0173, Test Loss = 5.8045\n",
      "Iteration 4800 : w = [17.3111  2.8182 -6.8758], b = 42.7562, Train Loss = 3.9907, Test Loss = 5.7817\n",
      "Iteration 4850 : w = [17.2854  2.6101 -7.0963], b = 42.6175, Train Loss = 4.0550, Test Loss = 5.8423\n",
      "Iteration 4900 : w = [17.443   2.6004 -7.0655], b = 42.4766, Train Loss = 4.0151, Test Loss = 5.6995\n",
      "Iteration 4950 : w = [17.4596  2.518  -7.0742], b = 42.4887, Train Loss = 4.0133, Test Loss = 5.7221\n",
      "Iteration 5000 : w = [17.5297  2.5275 -7.0262], b = 42.7097, Train Loss = 3.9517, Test Loss = 5.6742\n",
      "Iteration 5050 : w = [17.635   2.6949 -6.9442], b = 42.7280, Train Loss = 3.9896, Test Loss = 5.6147\n",
      "Iteration 5100 : w = [17.5571  2.5256 -6.8875], b = 42.7952, Train Loss = 3.9295, Test Loss = 5.7026\n",
      "Iteration 5150 : w = [17.5636  2.3988 -6.7782], b = 42.8895, Train Loss = 3.9667, Test Loss = 5.8582\n",
      "Iteration 5200 : w = [17.5863  2.4535 -6.739 ], b = 42.7431, Train Loss = 3.9158, Test Loss = 5.7383\n",
      "Iteration 5250 : w = [17.6147  2.5356 -6.7268], b = 42.7222, Train Loss = 3.9222, Test Loss = 5.6983\n",
      "Iteration 5300 : w = [17.5196  2.3128 -6.7736], b = 42.7320, Train Loss = 3.9583, Test Loss = 5.8443\n",
      "Iteration 5350 : w = [17.666   2.2471 -6.7909], b = 42.7467, Train Loss = 3.9219, Test Loss = 5.7540\n",
      "Iteration 5400 : w = [17.6471  2.1646 -6.5645], b = 42.7493, Train Loss = 4.0287, Test Loss = 5.9853\n",
      "Iteration 5450 : w = [17.6772  2.273  -6.683 ], b = 42.6763, Train Loss = 3.9238, Test Loss = 5.7533\n",
      "Iteration 5500 : w = [17.6668  2.2255 -6.8195], b = 42.7419, Train Loss = 3.9261, Test Loss = 5.7536\n",
      "Iteration 5550 : w = [17.8414  2.3493 -6.8447], b = 42.8095, Train Loss = 3.9215, Test Loss = 5.6238\n",
      "Iteration 5600 : w = [17.7954  2.2564 -6.8773], b = 42.7997, Train Loss = 3.9098, Test Loss = 5.6567\n",
      "Iteration 5650 : w = [17.8402  2.2778 -6.8212], b = 42.6744, Train Loss = 3.8879, Test Loss = 5.5860\n",
      "Iteration 5700 : w = [17.7891  2.2222 -6.935 ], b = 42.6966, Train Loss = 3.9028, Test Loss = 5.6211\n",
      "Iteration 5750 : w = [17.8545  2.2213 -7.0224], b = 42.6875, Train Loss = 3.9237, Test Loss = 5.5850\n",
      "Iteration 5800 : w = [17.7978  2.1896 -6.9605], b = 42.7459, Train Loss = 3.9194, Test Loss = 5.6498\n",
      "Iteration 5850 : w = [17.6218  2.0318 -6.8966], b = 42.5002, Train Loss = 4.0812, Test Loss = 5.8971\n",
      "Iteration 5900 : w = [17.8055  2.2255 -6.671 ], b = 42.6465, Train Loss = 3.9143, Test Loss = 5.6874\n",
      "Iteration 5950 : w = [17.8844  2.2917 -6.9316], b = 42.6050, Train Loss = 3.9064, Test Loss = 5.5288\n",
      "Iteration 6000 : w = [18.0237  2.3514 -7.0072], b = 42.8251, Train Loss = 4.0246, Test Loss = 5.5656\n",
      "Iteration 6050 : w = [17.872   2.2365 -7.1485], b = 42.8606, Train Loss = 4.0291, Test Loss = 5.6679\n",
      "Iteration 6100 : w = [17.7994  2.1677 -7.0285], b = 42.8643, Train Loss = 3.9817, Test Loss = 5.7243\n",
      "Iteration 6150 : w = [17.8991  2.2985 -6.8666], b = 42.6389, Train Loss = 3.9003, Test Loss = 5.5369\n",
      "Iteration 6200 : w = [17.9098  2.242  -6.7465], b = 42.7181, Train Loss = 3.9046, Test Loss = 5.6181\n",
      "Iteration 6250 : w = [17.8927  2.2065 -6.9241], b = 42.8455, Train Loss = 3.9332, Test Loss = 5.6401\n",
      "Iteration 6300 : w = [17.9021  2.3631 -7.0141], b = 42.5890, Train Loss = 3.9529, Test Loss = 5.5180\n",
      "Iteration 6350 : w = [17.7342  2.2146 -6.8269], b = 42.9463, Train Loss = 3.9868, Test Loss = 5.8459\n",
      "Iteration 6400 : w = [17.6009  2.0403 -6.5501], b = 42.4589, Train Loss = 4.1735, Test Loss = 6.0408\n",
      "Iteration 6450 : w = [17.7957  2.3202 -6.7708], b = 42.5576, Train Loss = 3.9004, Test Loss = 5.5840\n",
      "Iteration 6500 : w = [17.8388  2.3087 -7.0816], b = 42.5321, Train Loss = 3.9660, Test Loss = 5.5672\n",
      "Iteration 6550 : w = [17.9445  2.331  -6.928 ], b = 42.5075, Train Loss = 3.9512, Test Loss = 5.5070\n",
      "Iteration 6600 : w = [17.7687  2.2405 -6.4481], b = 42.7352, Train Loss = 4.0436, Test Loss = 5.9670\n",
      "Iteration 6650 : w = [17.6753  2.1947 -6.5477], b = 42.4745, Train Loss = 4.0363, Test Loss = 5.8414\n",
      "Iteration 6700 : w = [17.917   2.4038 -6.5695], b = 42.5256, Train Loss = 4.0283, Test Loss = 5.6748\n",
      "Iteration 6750 : w = [17.9675  2.3611 -6.6651], b = 42.6734, Train Loss = 3.9743, Test Loss = 5.6318\n",
      "Iteration 6800 : w = [17.9406  2.3673 -6.6247], b = 42.5507, Train Loss = 3.9905, Test Loss = 5.6282\n",
      "Iteration 6850 : w = [17.9677  2.2692 -6.8052], b = 42.5073, Train Loss = 3.9338, Test Loss = 5.5253\n",
      "Iteration 6900 : w = [17.987   2.4101 -6.6397], b = 42.7075, Train Loss = 4.0221, Test Loss = 5.6751\n",
      "Iteration 6950 : w = [17.9208  2.4189 -6.7012], b = 42.8612, Train Loss = 4.0099, Test Loss = 5.7225\n",
      "Iteration 7000 : w = [18.1647  2.3873 -6.792 ], b = 42.7889, Train Loss = 4.1173, Test Loss = 5.6381\n",
      "Iteration 7050 : w = [18.0306  2.0264 -6.6891], b = 42.8582, Train Loss = 3.9697, Test Loss = 5.7646\n",
      "Iteration 7100 : w = [18.07    2.094  -6.8104], b = 42.5669, Train Loss = 3.9184, Test Loss = 5.5356\n",
      "Iteration 7150 : w = [17.8102  1.9722 -6.7297], b = 42.4433, Train Loss = 4.0292, Test Loss = 5.7817\n",
      "Iteration 7200 : w = [17.8816  2.1453 -6.7928], b = 42.1658, Train Loss = 4.1322, Test Loss = 5.7024\n",
      "Iteration 7250 : w = [17.721   2.1854 -6.8709], b = 42.4984, Train Loss = 3.9392, Test Loss = 5.6641\n",
      "Iteration 7300 : w = [17.6853  2.3765 -6.8189], b = 42.5170, Train Loss = 3.9087, Test Loss = 5.6028\n",
      "Iteration 7350 : w = [17.7253  2.5504 -6.8717], b = 42.4767, Train Loss = 3.9640, Test Loss = 5.5528\n",
      "Iteration 7400 : w = [17.6861  2.3767 -7.0505], b = 42.4136, Train Loss = 3.9961, Test Loss = 5.6321\n",
      "Iteration 7450 : w = [17.6577  2.3094 -6.6105], b = 42.6057, Train Loss = 3.9495, Test Loss = 5.7728\n",
      "Iteration 7500 : w = [17.5099  2.362  -6.6253], b = 42.4106, Train Loss = 4.0355, Test Loss = 5.8169\n",
      "Iteration 7550 : w = [17.6102  2.4253 -6.6146], b = 42.6607, Train Loss = 3.9428, Test Loss = 5.7774\n",
      "Iteration 7600 : w = [17.7085  2.2655 -6.6932], b = 42.6792, Train Loss = 3.9156, Test Loss = 5.7295\n",
      "Iteration 7650 : w = [17.8814  2.2751 -7.0083], b = 42.4736, Train Loss = 3.9543, Test Loss = 5.5453\n",
      "Iteration 7700 : w = [17.5586  1.9856 -7.1569], b = 42.7444, Train Loss = 4.2703, Test Loss = 6.1573\n",
      "Iteration 7750 : w = [17.7549  2.2635 -6.8635], b = 42.6413, Train Loss = 3.8905, Test Loss = 5.6158\n",
      "Iteration 7800 : w = [17.5072  2.1852 -6.7898], b = 42.6725, Train Loss = 4.0274, Test Loss = 5.9311\n",
      "Iteration 7850 : w = [17.6898  2.3729 -6.7551], b = 42.6058, Train Loss = 3.8975, Test Loss = 5.6374\n",
      "Iteration 7900 : w = [17.4723  2.2545 -6.6837], b = 42.1922, Train Loss = 4.2392, Test Loss = 5.9570\n",
      "Iteration 7950 : w = [17.4334  2.4317 -6.9087], b = 42.2601, Train Loss = 4.1131, Test Loss = 5.8148\n",
      "Iteration 8000 : w = [17.7495  2.6794 -6.9596], b = 42.4771, Train Loss = 4.0612, Test Loss = 5.5696\n",
      "Iteration 8050 : w = [17.5915  2.5486 -6.5331], b = 42.9675, Train Loss = 4.0947, Test Loss = 6.0884\n",
      "Iteration 8100 : w = [17.4708  2.4401 -6.5354], b = 42.6798, Train Loss = 4.0157, Test Loss = 5.9571\n",
      "Iteration 8150 : w = [17.4613  2.4552 -6.4707], b = 42.5292, Train Loss = 4.0725, Test Loss = 5.9584\n",
      "Iteration 8200 : w = [17.8234  2.5243 -6.5791], b = 42.8124, Train Loss = 4.0478, Test Loss = 5.8216\n",
      "Iteration 8250 : w = [17.8947  2.4219 -6.7474], b = 42.5824, Train Loss = 3.9509, Test Loss = 5.5636\n",
      "Iteration 8300 : w = [17.7654  2.2259 -6.9827], b = 42.5329, Train Loss = 3.9307, Test Loss = 5.6127\n",
      "Iteration 8350 : w = [17.8093  2.5105 -7.0434], b = 42.8602, Train Loss = 4.0282, Test Loss = 5.6208\n",
      "Iteration 8400 : w = [17.4273  2.2845 -7.0479], b = 42.6698, Train Loss = 4.0738, Test Loss = 5.9258\n",
      "Iteration 8450 : w = [17.5119  2.4578 -6.8469], b = 42.7949, Train Loss = 3.9355, Test Loss = 5.7730\n",
      "Iteration 8500 : w = [17.3541  2.3691 -7.2698], b = 42.7813, Train Loss = 4.2462, Test Loss = 6.0895\n",
      "Iteration 8550 : w = [17.4488  2.5233 -7.3275], b = 42.4679, Train Loss = 4.2144, Test Loss = 5.9169\n",
      "Iteration 8600 : w = [17.8137  2.6947 -7.1138], b = 42.6314, Train Loss = 4.1437, Test Loss = 5.6025\n",
      "Iteration 8650 : w = [17.7074  2.7076 -7.1137], b = 42.5572, Train Loss = 4.0984, Test Loss = 5.6038\n",
      "Iteration 8700 : w = [17.8098  2.6005 -6.9366], b = 43.0455, Train Loss = 4.1574, Test Loss = 5.7928\n",
      "Iteration 8750 : w = [17.6444  2.4489 -6.9969], b = 42.7788, Train Loss = 3.9384, Test Loss = 5.6513\n",
      "Iteration 8800 : w = [17.4394  2.3397 -6.879 ], b = 42.5396, Train Loss = 3.9996, Test Loss = 5.8202\n",
      "Iteration 8850 : w = [17.7101  2.5051 -6.796 ], b = 42.7401, Train Loss = 3.9199, Test Loss = 5.6345\n",
      "Iteration 8900 : w = [17.611   2.274  -6.5098], b = 42.7355, Train Loss = 4.0298, Test Loss = 5.9912\n",
      "Iteration 8950 : w = [17.5518  2.4026 -6.7574], b = 42.5855, Train Loss = 3.9210, Test Loss = 5.7115\n",
      "Iteration 9000 : w = [17.7735  2.4818 -6.6808], b = 42.6363, Train Loss = 3.9426, Test Loss = 5.6393\n",
      "Iteration 9050 : w = [17.7158  2.2794 -6.6817], b = 42.7976, Train Loss = 3.9354, Test Loss = 5.7933\n",
      "Iteration 9100 : w = [17.4677  1.8491 -6.8232], b = 42.6279, Train Loss = 4.4196, Test Loss = 6.4089\n",
      "Iteration 9150 : w = [17.8855  2.3254 -6.6698], b = 42.6073, Train Loss = 3.9318, Test Loss = 5.6174\n",
      "Iteration 9200 : w = [17.7629  2.196  -6.5221], b = 42.4379, Train Loss = 4.0409, Test Loss = 5.7962\n",
      "Iteration 9250 : w = [17.8867  2.2511 -7.0955], b = 42.3570, Train Loss = 4.0461, Test Loss = 5.6212\n",
      "Iteration 9300 : w = [18.1516  2.3271 -7.0562], b = 42.7775, Train Loss = 4.0992, Test Loss = 5.5539\n",
      "Iteration 9350 : w = [18.0234  2.4376 -6.8281], b = 42.8186, Train Loss = 4.0469, Test Loss = 5.6209\n",
      "Iteration 9400 : w = [17.9399  2.5574 -6.7841], b = 42.9618, Train Loss = 4.1455, Test Loss = 5.7805\n",
      "Iteration 9450 : w = [17.8168  2.3002 -6.756 ], b = 42.6649, Train Loss = 3.8932, Test Loss = 5.6137\n",
      "Iteration 9500 : w = [17.5987  2.2309 -6.719 ], b = 42.5497, Train Loss = 3.9668, Test Loss = 5.7842\n",
      "Iteration 9550 : w = [17.6375  2.4651 -6.9165], b = 42.5023, Train Loss = 3.9270, Test Loss = 5.5900\n",
      "Iteration 9600 : w = [17.7518  2.3731 -6.6801], b = 42.2267, Train Loss = 4.0951, Test Loss = 5.6787\n",
      "Iteration 9650 : w = [18.0983  2.4125 -6.638 ], b = 42.4613, Train Loss = 4.1387, Test Loss = 5.6434\n",
      "Iteration 9700 : w = [17.9291  2.4393 -7.1589], b = 42.4923, Train Loss = 4.0905, Test Loss = 5.5783\n",
      "Iteration 9750 : w = [17.6715  2.1133 -6.8181], b = 42.9808, Train Loss = 4.0722, Test Loss = 6.0153\n",
      "Iteration 9800 : w = [18.0189  2.2405 -6.8759], b = 42.9497, Train Loss = 4.0099, Test Loss = 5.6759\n",
      "Iteration 9850 : w = [17.7459  2.004  -6.7681], b = 42.9224, Train Loss = 4.0648, Test Loss = 6.0049\n",
      "Iteration 9900 : w = [17.7132  2.1908 -6.5008], b = 42.7839, Train Loss = 4.0381, Test Loss = 6.0022\n",
      "Iteration 9950 : w = [17.7256  2.0563 -6.5177], b = 43.0168, Train Loss = 4.1958, Test Loss = 6.3024\n",
      "Iteration 10000 : w = [18.1112  2.229  -6.916 ], b = 42.5583, Train Loss = 3.9737, Test Loss = 5.4946\n",
      "Iteration 10050 : w = [18.1106  2.1903 -6.69  ], b = 42.5510, Train Loss = 3.9782, Test Loss = 5.5696\n",
      "Iteration 10100 : w = [17.9024  2.1782 -6.8697], b = 42.8838, Train Loss = 3.9434, Test Loss = 5.6856\n",
      "Iteration 10150 : w = [17.8073  2.3699 -6.592 ], b = 42.7448, Train Loss = 3.9622, Test Loss = 5.7612\n",
      "Iteration 10200 : w = [17.8357  2.4546 -6.7681], b = 42.5345, Train Loss = 3.9472, Test Loss = 5.5568\n",
      "Iteration 10250 : w = [17.5458  2.2184 -6.7025], b = 42.4621, Train Loss = 4.0311, Test Loss = 5.8434\n",
      "Iteration 10300 : w = [17.7026  2.334  -6.7649], b = 42.5047, Train Loss = 3.9159, Test Loss = 5.6238\n",
      "Iteration 10350 : w = [17.7331  2.3374 -6.7314], b = 42.1815, Train Loss = 4.1203, Test Loss = 5.7014\n",
      "Iteration 10400 : w = [17.4997  2.1285 -6.778 ], b = 42.8018, Train Loss = 4.0975, Test Loss = 6.0755\n",
      "Iteration 10450 : w = [17.6193  2.3694 -6.8602], b = 42.7709, Train Loss = 3.9142, Test Loss = 5.7129\n",
      "Iteration 10500 : w = [17.6931  2.1152 -6.9686], b = 42.4601, Train Loss = 4.0112, Test Loss = 5.7504\n",
      "Iteration 10550 : w = [18.0229  2.3669 -6.9833], b = 42.5512, Train Loss = 4.0062, Test Loss = 5.5046\n",
      "Iteration 10600 : w = [17.8074  2.1601 -7.0104], b = 42.5343, Train Loss = 3.9451, Test Loss = 5.6265\n",
      "Iteration 10650 : w = [17.7066  2.1635 -6.8   ], b = 42.3798, Train Loss = 4.0018, Test Loss = 5.7119\n",
      "Iteration 10700 : w = [17.8766  2.259  -6.8379], b = 42.6298, Train Loss = 3.8901, Test Loss = 5.5582\n",
      "Iteration 10750 : w = [17.6554  1.9842 -6.9036], b = 42.4097, Train Loss = 4.1309, Test Loss = 5.9179\n",
      "Iteration 10800 : w = [18.0103  2.0418 -6.7097], b = 42.6156, Train Loss = 3.9211, Test Loss = 5.6274\n",
      "Iteration 10850 : w = [18.0686  2.1931 -6.7684], b = 42.6552, Train Loss = 3.9322, Test Loss = 5.5524\n",
      "Iteration 10900 : w = [17.7857  2.0922 -6.8442], b = 42.7405, Train Loss = 3.9332, Test Loss = 5.7357\n",
      "Iteration 10950 : w = [17.8979  2.1562 -6.597 ], b = 42.6369, Train Loss = 3.9472, Test Loss = 5.7191\n",
      "Iteration 11000 : w = [17.9301  2.0511 -6.4312], b = 42.6314, Train Loss = 4.0682, Test Loss = 5.9187\n",
      "Iteration 11050 : w = [18.2237  2.2549 -6.8211], b = 43.2000, Train Loss = 4.3477, Test Loss = 5.9716\n",
      "Iteration 11100 : w = [17.7996  2.1297 -6.9914], b = 42.8741, Train Loss = 3.9828, Test Loss = 5.7532\n",
      "Iteration 11150 : w = [17.823   2.1769 -6.7134], b = 42.7631, Train Loss = 3.9189, Test Loss = 5.7279\n",
      "Iteration 11200 : w = [17.7406  2.2643 -6.7297], b = 42.8138, Train Loss = 3.9267, Test Loss = 5.7622\n",
      "Iteration 11250 : w = [17.6777  2.2787 -6.9396], b = 42.6030, Train Loss = 3.9158, Test Loss = 5.6470\n",
      "Iteration 11300 : w = [17.7604  2.4133 -6.8234], b = 42.3035, Train Loss = 4.0194, Test Loss = 5.5975\n",
      "Iteration 11350 : w = [17.5888  2.2147 -6.8326], b = 42.5135, Train Loss = 3.9770, Test Loss = 5.7660\n",
      "Iteration 11400 : w = [17.8604  2.2301 -6.9314], b = 42.6973, Train Loss = 3.8983, Test Loss = 5.5773\n",
      "Iteration 11450 : w = [17.5726  1.9987 -6.821 ], b = 42.6996, Train Loss = 4.1243, Test Loss = 6.0532\n",
      "Iteration 11500 : w = [17.7581  2.4214 -6.7298], b = 42.7967, Train Loss = 3.9289, Test Loss = 5.6946\n",
      "Iteration 11550 : w = [17.9351  2.5405 -6.8504], b = 42.8986, Train Loss = 4.0908, Test Loss = 5.6845\n",
      "Iteration 11600 : w = [17.7119  2.4129 -6.6328], b = 42.7671, Train Loss = 3.9442, Test Loss = 5.7697\n",
      "Iteration 11650 : w = [17.7879  2.3076 -6.5945], b = 42.5939, Train Loss = 3.9466, Test Loss = 5.7078\n",
      "Iteration 11700 : w = [17.7855  2.691  -6.8812], b = 42.4991, Train Loss = 4.0714, Test Loss = 5.5777\n",
      "Iteration 11750 : w = [17.8217  2.6339 -7.0398], b = 42.3952, Train Loss = 4.1321, Test Loss = 5.5948\n",
      "Iteration 11800 : w = [17.682   2.46   -6.8409], b = 42.5766, Train Loss = 3.9030, Test Loss = 5.5829\n",
      "Iteration 11850 : w = [17.7886  2.4189 -6.8232], b = 42.5460, Train Loss = 3.9145, Test Loss = 5.5500\n",
      "Iteration 11900 : w = [18.162   2.5425 -6.8687], b = 42.4644, Train Loss = 4.2808, Test Loss = 5.6401\n",
      "Iteration 11950 : w = [17.7469  2.2161 -6.9483], b = 42.6069, Train Loss = 3.9144, Test Loss = 5.6319\n",
      "Iteration 12000 : w = [17.8602  2.2659 -6.7286], b = 42.6730, Train Loss = 3.8999, Test Loss = 5.6219\n",
      "Iteration 12050 : w = [18.0623  2.4896 -6.8382], b = 42.7793, Train Loss = 4.1063, Test Loss = 5.6224\n",
      "Iteration 12100 : w = [18.007   2.4499 -6.7452], b = 42.7083, Train Loss = 4.0289, Test Loss = 5.6114\n",
      "Iteration 12150 : w = [17.8995  2.3422 -6.6061], b = 42.8138, Train Loss = 3.9891, Test Loss = 5.7710\n",
      "Iteration 12200 : w = [17.7538  2.1564 -6.4853], b = 42.5475, Train Loss = 4.0421, Test Loss = 5.8798\n",
      "Iteration 12250 : w = [17.8558  2.2307 -6.703 ], b = 42.2825, Train Loss = 4.0419, Test Loss = 5.6424\n",
      "Iteration 12300 : w = [18.0203  2.1975 -6.7028], b = 42.5858, Train Loss = 3.9344, Test Loss = 5.5765\n",
      "Iteration 12350 : w = [17.845   2.1423 -6.7752], b = 42.7397, Train Loss = 3.9070, Test Loss = 5.6849\n",
      "Iteration 12400 : w = [17.4835  2.0559 -6.8051], b = 42.8311, Train Loss = 4.1847, Test Loss = 6.1940\n",
      "Iteration 12450 : w = [17.606   2.2116 -6.4801], b = 42.5914, Train Loss = 4.0726, Test Loss = 5.9882\n",
      "Iteration 12500 : w = [17.6559  2.3549 -6.5271], b = 42.2784, Train Loss = 4.1262, Test Loss = 5.8035\n",
      "Iteration 12550 : w = [18.0071  2.611  -6.3571], b = 42.8079, Train Loss = 4.4151, Test Loss = 6.1891\n",
      "Iteration 12600 : w = [17.4735  2.1116 -6.7437], b = 42.6563, Train Loss = 4.1170, Test Loss = 6.0641\n",
      "Iteration 12650 : w = [17.4205  2.3313 -6.8573], b = 42.4160, Train Loss = 4.0576, Test Loss = 5.8507\n",
      "Iteration 12700 : w = [17.261   2.0951 -6.7458], b = 42.4600, Train Loss = 4.4168, Test Loss = 6.3744\n",
      "Iteration 12750 : w = [17.4471  2.2875 -6.786 ], b = 42.8120, Train Loss = 4.0316, Test Loss = 5.9888\n",
      "Iteration 12800 : w = [17.5549  2.4499 -6.7306], b = 42.7644, Train Loss = 3.9269, Test Loss = 5.7787\n",
      "Iteration 12850 : w = [17.5501  2.515  -6.6032], b = 42.5647, Train Loss = 3.9642, Test Loss = 5.7604\n",
      "Iteration 12900 : w = [17.8661  2.6541 -6.3855], b = 42.5123, Train Loss = 4.2996, Test Loss = 5.9600\n",
      "Iteration 12950 : w = [17.5651  2.4058 -6.5092], b = 42.7481, Train Loss = 4.0171, Test Loss = 5.9702\n",
      "Iteration 13000 : w = [17.4378  2.3586 -6.6547], b = 42.8205, Train Loss = 4.0309, Test Loss = 6.0291\n",
      "Iteration 13050 : w = [17.7707  2.4562 -6.6458], b = 42.8226, Train Loss = 3.9734, Test Loss = 5.7694\n",
      "Iteration 13100 : w = [17.8074  2.1529 -6.7496], b = 42.8508, Train Loss = 3.9456, Test Loss = 5.7877\n",
      "Iteration 13150 : w = [17.8674  2.1016 -6.7787], b = 42.8122, Train Loss = 3.9300, Test Loss = 5.7325\n",
      "Iteration 13200 : w = [17.8591  2.0756 -6.8162], b = 42.5130, Train Loss = 3.9310, Test Loss = 5.6396\n",
      "Iteration 13250 : w = [17.9838  2.2514 -6.6501], b = 42.4966, Train Loss = 3.9723, Test Loss = 5.5981\n",
      "Iteration 13300 : w = [17.8375  2.0963 -6.7874], b = 42.5748, Train Loss = 3.9175, Test Loss = 5.6556\n",
      "Iteration 13350 : w = [17.9264  2.2671 -6.9128], b = 42.7092, Train Loss = 3.9082, Test Loss = 5.5460\n",
      "Iteration 13400 : w = [17.9261  2.307  -6.8839], b = 42.8383, Train Loss = 3.9444, Test Loss = 5.6077\n",
      "Iteration 13450 : w = [17.7991  2.3255 -6.8703], b = 42.5009, Train Loss = 3.9126, Test Loss = 5.5529\n",
      "Iteration 13500 : w = [17.7272  2.2645 -6.6822], b = 42.6212, Train Loss = 3.9160, Test Loss = 5.7014\n",
      "Iteration 13550 : w = [18.016   2.5159 -7.1417], b = 42.8393, Train Loss = 4.1964, Test Loss = 5.6259\n",
      "Iteration 13600 : w = [17.8237  2.352  -6.9549], b = 42.6276, Train Loss = 3.9097, Test Loss = 5.5409\n",
      "Iteration 13650 : w = [17.8087  2.3977 -7.0885], b = 42.5281, Train Loss = 3.9818, Test Loss = 5.5622\n",
      "Iteration 13700 : w = [17.9061  2.3618 -6.7125], b = 42.6059, Train Loss = 3.9364, Test Loss = 5.5839\n",
      "Iteration 13750 : w = [17.832   2.2602 -6.6093], b = 42.8273, Train Loss = 3.9665, Test Loss = 5.8158\n",
      "Iteration 13800 : w = [17.8722  2.2146 -6.5928], b = 42.6245, Train Loss = 3.9466, Test Loss = 5.7095\n",
      "Iteration 13850 : w = [17.9386  2.1486 -6.7662], b = 42.6599, Train Loss = 3.8969, Test Loss = 5.6016\n",
      "Iteration 13900 : w = [17.8287  1.9856 -6.6562], b = 43.0008, Train Loss = 4.1092, Test Loss = 6.0958\n",
      "Iteration 13950 : w = [17.9189  2.182  -7.0575], b = 42.9465, Train Loss = 4.0253, Test Loss = 5.6982\n",
      "Iteration 14000 : w = [17.9128  2.3631 -7.1142], b = 42.9791, Train Loss = 4.1034, Test Loss = 5.6869\n",
      "Iteration 14050 : w = [17.8909  2.3504 -6.6734], b = 43.0927, Train Loss = 4.1291, Test Loss = 5.9843\n",
      "Iteration 14100 : w = [17.6492  2.0751 -6.7986], b = 42.6704, Train Loss = 4.0023, Test Loss = 5.8676\n",
      "Iteration 14150 : w = [17.8966  2.3638 -6.8017], b = 42.6105, Train Loss = 3.9190, Test Loss = 5.5438\n",
      "Iteration 14200 : w = [17.9506  2.1893 -7.1416], b = 42.7073, Train Loss = 3.9901, Test Loss = 5.5908\n",
      "Iteration 14250 : w = [18.1383  2.1163 -7.0471], b = 42.6234, Train Loss = 3.9784, Test Loss = 5.5140\n",
      "Iteration 14300 : w = [18.0068  1.9565 -6.8936], b = 42.6461, Train Loss = 3.9256, Test Loss = 5.6228\n",
      "Iteration 14350 : w = [18.0317  1.7952 -6.7026], b = 42.7892, Train Loss = 4.0232, Test Loss = 5.8590\n",
      "Iteration 14400 : w = [18.0523  1.7399 -6.8708], b = 42.5026, Train Loss = 4.0390, Test Loss = 5.7511\n",
      "Iteration 14450 : w = [18.0922  1.9701 -7.2286], b = 42.5294, Train Loss = 4.0850, Test Loss = 5.6797\n",
      "Iteration 14500 : w = [18.165   2.1267 -6.8433], b = 42.4388, Train Loss = 3.9962, Test Loss = 5.5152\n",
      "Iteration 14550 : w = [18.0145  2.2505 -6.8248], b = 42.5353, Train Loss = 3.9367, Test Loss = 5.5145\n",
      "Iteration 14600 : w = [17.7021  2.1129 -6.9608], b = 42.7162, Train Loss = 3.9720, Test Loss = 5.7722\n",
      "Iteration 14650 : w = [17.5667  2.0606 -7.0886], b = 42.7975, Train Loss = 4.1651, Test Loss = 6.0431\n",
      "Iteration 14700 : w = [17.7546  2.1719 -6.4873], b = 42.8907, Train Loss = 4.0807, Test Loss = 6.0921\n",
      "Iteration 14750 : w = [17.5371  2.3066 -6.7082], b = 42.6864, Train Loss = 3.9581, Test Loss = 5.8418\n",
      "Iteration 14800 : w = [17.6737  2.4884 -6.7723], b = 42.5190, Train Loss = 3.9233, Test Loss = 5.5965\n",
      "Iteration 14850 : w = [17.8565  2.7825 -6.6734], b = 42.8827, Train Loss = 4.2705, Test Loss = 5.9010\n",
      "Iteration 14900 : w = [17.7754  2.6996 -6.8777], b = 42.6511, Train Loss = 4.0475, Test Loss = 5.5948\n",
      "Iteration 14950 : w = [17.5794  2.3916 -6.6159], b = 42.7871, Train Loss = 3.9680, Test Loss = 5.8861\n",
      "Iteration 15000 : w = [17.6915  2.5429 -6.5081], b = 42.6425, Train Loss = 4.0253, Test Loss = 5.8192\n",
      "Iteration 15050 : w = [17.6576  2.5162 -6.8587], b = 42.6464, Train Loss = 3.9069, Test Loss = 5.5951\n",
      "Iteration 15100 : w = [17.5088  2.5288 -7.0855], b = 42.6178, Train Loss = 3.9806, Test Loss = 5.6829\n",
      "Iteration 15150 : w = [17.3193  2.3397 -6.8154], b = 42.3163, Train Loss = 4.1862, Test Loss = 5.9879\n",
      "Iteration 15200 : w = [17.2202  1.9937 -6.8492], b = 42.1635, Train Loss = 4.8207, Test Loss = 6.6876\n",
      "Iteration 15250 : w = [17.715   2.2692 -6.8739], b = 42.5915, Train Loss = 3.9004, Test Loss = 5.6269\n",
      "Iteration 15300 : w = [17.8279  2.3324 -6.9466], b = 43.0610, Train Loss = 4.0701, Test Loss = 5.8049\n",
      "Iteration 15350 : w = [17.8265  2.4139 -7.0821], b = 42.4983, Train Loss = 3.9955, Test Loss = 5.5566\n",
      "Iteration 15400 : w = [18.0752  2.5257 -7.054 ], b = 42.7299, Train Loss = 4.1839, Test Loss = 5.5909\n",
      "Iteration 15450 : w = [17.9469  2.1999 -6.9398], b = 42.6855, Train Loss = 3.9060, Test Loss = 5.5454\n",
      "Iteration 15500 : w = [17.7876  2.0804 -6.6461], b = 42.6910, Train Loss = 3.9645, Test Loss = 5.8177\n",
      "Iteration 15550 : w = [17.9869  2.3651 -6.5925], b = 42.8582, Train Loss = 4.0581, Test Loss = 5.8096\n",
      "Iteration 15600 : w = [18.013   2.2778 -6.7558], b = 42.6699, Train Loss = 3.9380, Test Loss = 5.5657\n",
      "Iteration 15650 : w = [17.6988  2.0671 -6.8846], b = 42.5430, Train Loss = 3.9925, Test Loss = 5.7767\n",
      "Iteration 15700 : w = [17.6907  2.1674 -7.1359], b = 42.6234, Train Loss = 4.0270, Test Loss = 5.7658\n",
      "Iteration 15750 : w = [17.6254  2.2348 -6.8143], b = 42.5284, Train Loss = 3.9470, Test Loss = 5.7210\n",
      "Iteration 15800 : w = [17.8561  2.1679 -6.7468], b = 42.5387, Train Loss = 3.9117, Test Loss = 5.6157\n",
      "Iteration 15850 : w = [17.9461  2.1002 -6.7169], b = 42.7181, Train Loss = 3.9139, Test Loss = 5.6672\n",
      "Iteration 15900 : w = [17.8955  2.0758 -6.6259], b = 42.6353, Train Loss = 3.9474, Test Loss = 5.7336\n",
      "Iteration 15950 : w = [17.9346  2.1373 -6.8066], b = 42.4740, Train Loss = 3.9256, Test Loss = 5.5682\n",
      "Iteration 16000 : w = [17.8541  2.2275 -6.9155], b = 42.8203, Train Loss = 3.9211, Test Loss = 5.6386\n",
      "Iteration 16050 : w = [17.7279  2.1862 -6.6021], b = 42.6868, Train Loss = 3.9633, Test Loss = 5.8292\n",
      "Iteration 16100 : w = [17.843   2.3633 -6.453 ], b = 42.7099, Train Loss = 4.0503, Test Loss = 5.8847\n",
      "Iteration 16150 : w = [17.6369  2.0254 -6.9616], b = 42.5218, Train Loss = 4.0824, Test Loss = 5.8927\n",
      "Iteration 16200 : w = [17.927   2.2869 -6.6507], b = 42.2958, Train Loss = 4.0668, Test Loss = 5.6309\n",
      "Iteration 16250 : w = [18.147   2.1496 -6.6191], b = 42.7190, Train Loss = 4.0028, Test Loss = 5.6751\n",
      "Iteration 16300 : w = [18.1686  2.1137 -6.6999], b = 42.6680, Train Loss = 3.9685, Test Loss = 5.5920\n",
      "Iteration 16350 : w = [17.9625  2.1377 -6.7155], b = 42.6862, Train Loss = 3.9097, Test Loss = 5.6315\n",
      "Iteration 16400 : w = [18.0222  2.1666 -6.4492], b = 42.8706, Train Loss = 4.1052, Test Loss = 5.9909\n",
      "Iteration 16450 : w = [18.0424  2.1809 -6.7289], b = 42.6429, Train Loss = 3.9271, Test Loss = 5.5743\n",
      "Iteration 16500 : w = [17.9246  2.0141 -6.8454], b = 42.5543, Train Loss = 3.9270, Test Loss = 5.6310\n",
      "Iteration 16550 : w = [17.5256  1.8419 -7.1692], b = 42.3239, Train Loss = 4.5857, Test Loss = 6.4637\n",
      "Iteration 16600 : w = [17.7765  2.3933 -6.8706], b = 42.3586, Train Loss = 3.9835, Test Loss = 5.5718\n",
      "Iteration 16650 : w = [17.8063  2.4365 -6.8485], b = 42.4597, Train Loss = 3.9501, Test Loss = 5.5398\n",
      "Iteration 16700 : w = [17.6327  2.3438 -6.9451], b = 42.5348, Train Loss = 3.9279, Test Loss = 5.6402\n",
      "Iteration 16750 : w = [17.8245  2.5285 -6.9666], b = 42.8621, Train Loss = 4.0187, Test Loss = 5.6239\n",
      "Iteration 16800 : w = [17.7848  2.4078 -6.9439], b = 42.6832, Train Loss = 3.9122, Test Loss = 5.5581\n",
      "Iteration 16850 : w = [17.5162  2.1882 -6.9043], b = 42.6413, Train Loss = 4.0246, Test Loss = 5.8841\n",
      "Iteration 16900 : w = [17.474   2.2109 -6.8228], b = 42.5545, Train Loss = 4.0436, Test Loss = 5.9033\n",
      "Iteration 16950 : w = [17.7118  2.4117 -6.8678], b = 42.6696, Train Loss = 3.8932, Test Loss = 5.5951\n",
      "Iteration 17000 : w = [18.0042  2.6357 -6.5497], b = 42.7873, Train Loss = 4.2810, Test Loss = 5.9182\n",
      "Iteration 17050 : w = [17.6902  2.2594 -6.7533], b = 42.4635, Train Loss = 3.9438, Test Loss = 5.6689\n",
      "Iteration 17100 : w = [17.7649  2.2568 -6.76  ], b = 42.3886, Train Loss = 3.9643, Test Loss = 5.6278\n",
      "Iteration 17150 : w = [17.6542  2.1297 -6.885 ], b = 42.3658, Train Loss = 4.0523, Test Loss = 5.7843\n",
      "Iteration 17200 : w = [17.8914  2.6055 -7.1302], b = 42.7069, Train Loss = 4.1366, Test Loss = 5.5936\n",
      "Iteration 17250 : w = [17.3231  2.3297 -6.6828], b = 42.6309, Train Loss = 4.0941, Test Loss = 6.0572\n",
      "Iteration 17300 : w = [17.5341  2.4787 -6.7859], b = 42.5162, Train Loss = 3.9287, Test Loss = 5.6698\n",
      "Iteration 17350 : w = [17.7149  2.538  -6.8187], b = 42.4758, Train Loss = 3.9554, Test Loss = 5.5650\n",
      "Iteration 17400 : w = [17.5676  2.3974 -6.9846], b = 42.4181, Train Loss = 3.9888, Test Loss = 5.6802\n",
      "Iteration 17450 : w = [17.723   2.4437 -6.7721], b = 42.6819, Train Loss = 3.9021, Test Loss = 5.6216\n",
      "Iteration 17500 : w = [17.7036  2.2865 -6.8458], b = 42.6133, Train Loss = 3.8959, Test Loss = 5.6354\n",
      "Iteration 17550 : w = [18.0329  2.1318 -7.1071], b = 42.6237, Train Loss = 3.9779, Test Loss = 5.5544\n",
      "Iteration 17600 : w = [17.9416  2.094  -6.8301], b = 42.5855, Train Loss = 3.9014, Test Loss = 5.5829\n",
      "Iteration 17650 : w = [17.9107  2.1586 -6.6963], b = 42.5670, Train Loss = 3.9172, Test Loss = 5.6213\n",
      "Iteration 17700 : w = [18.03    2.0093 -6.6667], b = 42.4731, Train Loss = 3.9702, Test Loss = 5.6370\n",
      "Iteration 17750 : w = [18.0236  1.9145 -6.943 ], b = 42.7103, Train Loss = 3.9485, Test Loss = 5.6552\n",
      "Iteration 17800 : w = [18.1912  2.2463 -6.6151], b = 42.8509, Train Loss = 4.1107, Test Loss = 5.7757\n",
      "Iteration 17850 : w = [17.9866  1.8007 -6.7969], b = 42.5573, Train Loss = 4.0122, Test Loss = 5.7629\n",
      "Iteration 17900 : w = [17.9232  1.7063 -6.5094], b = 42.4218, Train Loss = 4.2679, Test Loss = 6.0709\n",
      "Iteration 17950 : w = [18.1438  2.0311 -6.6396], b = 42.4602, Train Loss = 4.0021, Test Loss = 5.6112\n",
      "Iteration 18000 : w = [17.8062  1.8847 -6.9919], b = 42.3748, Train Loss = 4.1430, Test Loss = 5.8834\n",
      "Iteration 18050 : w = [17.9583  1.9382 -7.0128], b = 42.2755, Train Loss = 4.1133, Test Loss = 5.7507\n",
      "Iteration 18100 : w = [17.9802  1.9618 -6.8336], b = 42.2443, Train Loss = 4.0920, Test Loss = 5.7000\n",
      "Iteration 18150 : w = [17.7309  1.9369 -7.0766], b = 42.2067, Train Loss = 4.3107, Test Loss = 6.0398\n",
      "Iteration 18200 : w = [17.9114  2.0184 -6.9272], b = 42.6118, Train Loss = 3.9289, Test Loss = 5.6375\n",
      "Iteration 18250 : w = [17.7048  1.9133 -6.6009], b = 42.3392, Train Loss = 4.2355, Test Loss = 6.0214\n",
      "Iteration 18300 : w = [17.5899  2.1128 -6.412 ], b = 42.2243, Train Loss = 4.3690, Test Loss = 6.1225\n",
      "Iteration 18350 : w = [17.6108  2.2663 -6.6269], b = 42.4886, Train Loss = 3.9927, Test Loss = 5.7918\n",
      "Iteration 18400 : w = [17.6691  2.3787 -6.7716], b = 42.4521, Train Loss = 3.9353, Test Loss = 5.6245\n",
      "Iteration 18450 : w = [17.7204  2.6551 -6.5351], b = 42.3448, Train Loss = 4.1698, Test Loss = 5.7591\n",
      "Iteration 18500 : w = [17.6807  2.5987 -6.6395], b = 42.5535, Train Loss = 3.9865, Test Loss = 5.6726\n",
      "Iteration 18550 : w = [17.3419  2.0479 -7.1508], b = 42.1357, Train Loss = 4.7080, Test Loss = 6.5606\n",
      "Iteration 18600 : w = [17.6341  2.5595 -7.0388], b = 42.6018, Train Loss = 3.9604, Test Loss = 5.5889\n",
      "Iteration 18650 : w = [17.7508  2.5974 -6.8536], b = 42.6312, Train Loss = 3.9642, Test Loss = 5.5687\n",
      "Iteration 18700 : w = [17.6122  2.2463 -6.8966], b = 42.3730, Train Loss = 4.0164, Test Loss = 5.7321\n",
      "Iteration 18750 : w = [17.8577  2.209  -6.765 ], b = 42.6991, Train Loss = 3.8941, Test Loss = 5.6331\n",
      "Iteration 18800 : w = [17.6907  2.1652 -6.7684], b = 42.3011, Train Loss = 4.0594, Test Loss = 5.7512\n",
      "Iteration 18850 : w = [17.543   1.8752 -6.8708], b = 42.2493, Train Loss = 4.4543, Test Loss = 6.2647\n",
      "Iteration 18900 : w = [17.8922  2.3226 -6.8188], b = 42.4263, Train Loss = 3.9549, Test Loss = 5.5388\n",
      "Iteration 18950 : w = [17.9014  2.3004 -6.4434], b = 42.6203, Train Loss = 4.0560, Test Loss = 5.8389\n",
      "Iteration 19000 : w = [17.7348  1.989  -6.8469], b = 42.2990, Train Loss = 4.1323, Test Loss = 5.8518\n",
      "Iteration 19050 : w = [17.8528  2.2263 -6.7533], b = 42.5422, Train Loss = 3.9057, Test Loss = 5.5919\n",
      "Iteration 19100 : w = [18.0302  2.3724 -6.7762], b = 42.8219, Train Loss = 4.0157, Test Loss = 5.6355\n",
      "Iteration 19150 : w = [18.1492  2.3936 -6.9033], b = 42.5606, Train Loss = 4.0991, Test Loss = 5.5374\n",
      "Iteration 19200 : w = [17.8022  2.1799 -7.0246], b = 42.5966, Train Loss = 3.9361, Test Loss = 5.6231\n",
      "Iteration 19250 : w = [17.8775  2.2199 -6.9697], b = 42.6588, Train Loss = 3.9060, Test Loss = 5.5618\n",
      "Iteration 19300 : w = [17.9061  2.2003 -7.1094], b = 42.5586, Train Loss = 3.9738, Test Loss = 5.5828\n",
      "Iteration 19350 : w = [17.8902  2.2379 -6.9717], b = 42.4350, Train Loss = 3.9561, Test Loss = 5.5527\n",
      "Iteration 19400 : w = [17.7504  2.2875 -6.7772], b = 42.2286, Train Loss = 4.0714, Test Loss = 5.6746\n",
      "Iteration 19450 : w = [17.6537  2.29   -6.9876], b = 42.3699, Train Loss = 4.0110, Test Loss = 5.6874\n",
      "Iteration 19500 : w = [17.7932  2.5639 -6.8474], b = 42.7189, Train Loss = 3.9680, Test Loss = 5.5885\n",
      "Iteration 19550 : w = [17.7732  2.6301 -6.989 ], b = 42.5979, Train Loss = 4.0202, Test Loss = 5.5537\n",
      "Iteration 19600 : w = [17.8295  2.5958 -6.8881], b = 42.5156, Train Loss = 4.0239, Test Loss = 5.5472\n",
      "Iteration 19650 : w = [17.4659  2.1612 -6.8404], b = 42.6112, Train Loss = 4.0791, Test Loss = 5.9743\n",
      "Iteration 19700 : w = [17.632   2.184  -6.9094], b = 42.4996, Train Loss = 3.9806, Test Loss = 5.7428\n",
      "Iteration 19750 : w = [17.9361  2.487  -6.8192], b = 42.7568, Train Loss = 4.0055, Test Loss = 5.5973\n",
      "Iteration 19800 : w = [17.8889  2.5331 -6.4656], b = 43.1122, Train Loss = 4.3458, Test Loss = 6.2921\n",
      "Iteration 19850 : w = [17.7913  2.2992 -6.646 ], b = 42.7990, Train Loss = 3.9419, Test Loss = 5.7704\n",
      "Iteration 19900 : w = [17.8709  2.1578 -6.5081], b = 42.7823, Train Loss = 4.0136, Test Loss = 5.9042\n",
      "Iteration 19950 : w = [17.9295  2.2046 -6.9757], b = 43.2015, Train Loss = 4.2115, Test Loss = 5.9499\n",
      "Iteration 20000 : w = [17.7942  2.001  -6.9257], b = 42.8865, Train Loss = 4.0297, Test Loss = 5.8734\n",
      "Iteration 20050 : w = [17.7407  1.9974 -6.8552], b = 42.5146, Train Loss = 4.0169, Test Loss = 5.7987\n",
      "Iteration 20100 : w = [18.0168  2.1783 -6.9004], b = 42.4446, Train Loss = 3.9543, Test Loss = 5.5206\n",
      "Iteration 20150 : w = [18.09    2.188  -6.9273], b = 42.8514, Train Loss = 3.9807, Test Loss = 5.5836\n",
      "Iteration 20200 : w = [18.0507  2.2447 -6.7369], b = 42.5275, Train Loss = 3.9623, Test Loss = 5.5438\n",
      "Iteration 20250 : w = [17.8904  2.03   -6.6764], b = 42.4399, Train Loss = 3.9888, Test Loss = 5.6978\n",
      "Iteration 20300 : w = [18.1326  2.0948 -6.7036], b = 42.5008, Train Loss = 3.9711, Test Loss = 5.5634\n",
      "Iteration 20350 : w = [17.7494  1.7258 -7.0224], b = 42.5330, Train Loss = 4.2963, Test Loss = 6.1448\n",
      "Iteration 20400 : w = [17.8786  2.0447 -6.8602], b = 42.7280, Train Loss = 3.9228, Test Loss = 5.6852\n",
      "Iteration 20450 : w = [17.8738  2.1406 -6.4979], b = 42.5666, Train Loss = 4.0136, Test Loss = 5.8059\n",
      "Iteration 20500 : w = [17.8414  2.0841 -6.5778], b = 42.6659, Train Loss = 3.9766, Test Loss = 5.8172\n",
      "Iteration 20550 : w = [17.9756  2.3449 -7.0202], b = 42.6744, Train Loss = 3.9730, Test Loss = 5.5177\n",
      "Iteration 20600 : w = [17.8287  2.3283 -6.7957], b = 42.8737, Train Loss = 3.9411, Test Loss = 5.6980\n",
      "Iteration 20650 : w = [17.9564  2.3258 -6.7463], b = 42.6932, Train Loss = 3.9347, Test Loss = 5.5858\n",
      "Iteration 20700 : w = [18.0335  2.4161 -6.9881], b = 42.4902, Train Loss = 4.0600, Test Loss = 5.5220\n",
      "Iteration 20750 : w = [17.7425  2.2061 -6.7641], b = 42.5787, Train Loss = 3.9114, Test Loss = 5.6676\n",
      "Iteration 20800 : w = [17.7067  2.2661 -6.6694], b = 42.5580, Train Loss = 3.9314, Test Loss = 5.7044\n",
      "Iteration 20850 : w = [17.9171  2.3    -7.0064], b = 42.3974, Train Loss = 3.9985, Test Loss = 5.5489\n",
      "Iteration 20900 : w = [17.584   1.8852 -7.2185], b = 42.4459, Train Loss = 4.4326, Test Loss = 6.2963\n",
      "Iteration 20950 : w = [17.9208  2.0537 -7.3887], b = 42.8010, Train Loss = 4.2364, Test Loss = 5.8789\n",
      "Iteration 21000 : w = [17.9592  2.2998 -6.7179], b = 42.6535, Train Loss = 3.9316, Test Loss = 5.5878\n",
      "Iteration 21050 : w = [17.7318  2.3319 -6.8514], b = 42.7195, Train Loss = 3.8919, Test Loss = 5.6316\n",
      "Iteration 21100 : w = [17.4874  2.1615 -6.9464], b = 42.8827, Train Loss = 4.1269, Test Loss = 6.0711\n",
      "Iteration 21150 : w = [17.7173  2.3789 -6.707 ], b = 42.6371, Train Loss = 3.9046, Test Loss = 5.6559\n",
      "Iteration 21200 : w = [17.9877  2.3325 -6.8159], b = 42.6954, Train Loss = 3.9428, Test Loss = 5.5502\n",
      "Iteration 21250 : w = [17.8276  2.0516 -6.7936], b = 42.3443, Train Loss = 4.0252, Test Loss = 5.7044\n",
      "Iteration 21300 : w = [17.9448  2.2044 -7.1146], b = 42.4786, Train Loss = 4.0025, Test Loss = 5.5805\n",
      "Iteration 21350 : w = [17.8311  2.1491 -7.1954], b = 42.6828, Train Loss = 4.0303, Test Loss = 5.7021\n",
      "Iteration 21400 : w = [18.0672  2.142  -6.9184], b = 42.6068, Train Loss = 3.9236, Test Loss = 5.5097\n",
      "Iteration 21450 : w = [18.0621  2.0496 -7.1667], b = 42.8141, Train Loss = 4.0410, Test Loss = 5.6400\n",
      "Iteration 21500 : w = [18.0693  2.1075 -7.0412], b = 42.9940, Train Loss = 4.0674, Test Loss = 5.6950\n",
      "Iteration 21550 : w = [17.9556  2.2103 -6.7429], b = 42.8376, Train Loss = 3.9396, Test Loss = 5.6808\n",
      "Iteration 21600 : w = [18.028   2.3603 -6.5849], b = 42.7972, Train Loss = 4.0618, Test Loss = 5.7716\n",
      "Iteration 21650 : w = [17.8734  1.9852 -6.7099], b = 43.1003, Train Loss = 4.1557, Test Loss = 6.1280\n",
      "Iteration 21700 : w = [17.8773  2.0496 -6.5345], b = 42.6288, Train Loss = 4.0042, Test Loss = 5.8367\n",
      "Iteration 21750 : w = [17.9874  2.4005 -6.6706], b = 42.6716, Train Loss = 4.0028, Test Loss = 5.6327\n",
      "Iteration 21800 : w = [17.8643  2.0333 -6.8834], b = 42.8060, Train Loss = 3.9501, Test Loss = 5.7384\n",
      "Iteration 21850 : w = [18.2336  2.3182 -6.8583], b = 42.6501, Train Loss = 4.1035, Test Loss = 5.5565\n",
      "Iteration 21900 : w = [18.1167  2.2514 -6.9024], b = 42.3995, Train Loss = 4.0405, Test Loss = 5.5204\n",
      "Iteration 21950 : w = [17.9245  1.8471 -6.8537], b = 42.5477, Train Loss = 4.0108, Test Loss = 5.7637\n",
      "Iteration 22000 : w = [17.8247  1.8171 -6.7661], b = 42.4051, Train Loss = 4.1468, Test Loss = 5.9165\n",
      "Iteration 22050 : w = [17.8425  1.9605 -6.7374], b = 42.5705, Train Loss = 3.9823, Test Loss = 5.7695\n",
      "Iteration 22100 : w = [18.0186  2.3292 -6.8402], b = 42.8169, Train Loss = 3.9808, Test Loss = 5.5952\n",
      "Iteration 22150 : w = [17.9304  2.3581 -6.7586], b = 42.6743, Train Loss = 3.9329, Test Loss = 5.5757\n",
      "Iteration 22200 : w = [17.7999  2.1575 -6.8553], b = 42.7210, Train Loss = 3.9058, Test Loss = 5.6699\n",
      "Iteration 22250 : w = [17.8072  2.0468 -6.7884], b = 42.7317, Train Loss = 3.9449, Test Loss = 5.7658\n",
      "Iteration 22300 : w = [17.8495  2.1925 -6.7986], b = 42.9608, Train Loss = 3.9840, Test Loss = 5.8045\n",
      "Iteration 22350 : w = [17.7445  2.5333 -7.0195], b = 42.6805, Train Loss = 3.9649, Test Loss = 5.5622\n",
      "Iteration 22400 : w = [17.8883  2.5232 -6.5651], b = 42.7574, Train Loss = 4.0750, Test Loss = 5.7973\n",
      "Iteration 22450 : w = [17.8275  2.4044 -6.6635], b = 42.8381, Train Loss = 3.9715, Test Loss = 5.7519\n",
      "Iteration 22500 : w = [17.6777  2.4403 -6.858 ], b = 42.6288, Train Loss = 3.8953, Test Loss = 5.5956\n",
      "Iteration 22550 : w = [17.8545  2.8678 -7.129 ], b = 42.8110, Train Loss = 4.3945, Test Loss = 5.7582\n",
      "Iteration 22600 : w = [17.6749  2.7172 -7.2892], b = 42.6176, Train Loss = 4.2099, Test Loss = 5.7084\n",
      "Iteration 22650 : w = [17.447   2.5633 -7.0706], b = 42.8041, Train Loss = 4.0075, Test Loss = 5.7694\n",
      "Iteration 22700 : w = [17.5381  2.556  -6.9539], b = 42.7311, Train Loss = 3.9323, Test Loss = 5.6644\n",
      "Iteration 22750 : w = [17.4775  2.732  -6.6364], b = 42.9170, Train Loss = 4.0583, Test Loss = 5.9655\n",
      "Iteration 22800 : w = [17.4334  2.9894 -6.6543], b = 43.3135, Train Loss = 4.5584, Test Loss = 6.5541\n",
      "Iteration 22850 : w = [17.5584  2.853  -6.4932], b = 42.9544, Train Loss = 4.2491, Test Loss = 6.1760\n",
      "Iteration 22900 : w = [17.1971  2.4715 -6.3851], b = 42.6494, Train Loss = 4.2779, Test Loss = 6.3895\n",
      "Iteration 22950 : w = [17.6917  2.8323 -6.7652], b = 42.9319, Train Loss = 4.1864, Test Loss = 5.8598\n",
      "Iteration 23000 : w = [17.4914  2.2787 -6.9271], b = 42.5484, Train Loss = 4.0035, Test Loss = 5.8135\n",
      "Iteration 23050 : w = [17.3036  2.1476 -6.7654], b = 42.7090, Train Loss = 4.2632, Test Loss = 6.2956\n",
      "Iteration 23100 : w = [17.4229  2.2578 -6.758 ], b = 42.8587, Train Loss = 4.0874, Test Loss = 6.0959\n",
      "Iteration 23150 : w = [17.5623  2.3705 -6.8677], b = 42.5933, Train Loss = 3.9200, Test Loss = 5.6902\n",
      "Iteration 23200 : w = [17.9253  2.8216 -6.9282], b = 42.9626, Train Loss = 4.4107, Test Loss = 5.8600\n",
      "Iteration 23250 : w = [17.6313  2.6951 -6.9633], b = 42.4879, Train Loss = 4.0151, Test Loss = 5.5817\n",
      "Iteration 23300 : w = [17.4008  2.4165 -7.0142], b = 42.6718, Train Loss = 4.0106, Test Loss = 5.8368\n",
      "Iteration 23350 : w = [17.6474  2.4193 -6.7468], b = 42.7034, Train Loss = 3.9024, Test Loss = 5.6867\n",
      "Iteration 23400 : w = [17.4191  2.3128 -6.6818], b = 42.8792, Train Loss = 4.0804, Test Loss = 6.1166\n",
      "Iteration 23450 : w = [17.5116  2.5135 -6.9652], b = 43.0798, Train Loss = 4.1122, Test Loss = 5.9609\n",
      "Iteration 23500 : w = [17.4832  2.2248 -7.1659], b = 42.7857, Train Loss = 4.1544, Test Loss = 6.0005\n",
      "Iteration 23550 : w = [17.7341  2.8103 -7.2629], b = 42.6628, Train Loss = 4.2942, Test Loss = 5.7180\n",
      "Iteration 23600 : w = [17.5659  2.5831 -6.8637], b = 42.7000, Train Loss = 3.9180, Test Loss = 5.6448\n",
      "Iteration 23650 : w = [17.8226  2.9352 -6.9798], b = 42.7032, Train Loss = 4.3622, Test Loss = 5.7535\n",
      "Iteration 23700 : w = [17.3443  2.3476 -6.8925], b = 42.2182, Train Loss = 4.2417, Test Loss = 5.9934\n",
      "Iteration 23750 : w = [17.7381  2.7147 -6.5606], b = 42.3257, Train Loss = 4.2189, Test Loss = 5.7639\n",
      "Iteration 23800 : w = [17.4631  2.3959 -6.8234], b = 42.4711, Train Loss = 3.9809, Test Loss = 5.7587\n",
      "Iteration 23850 : w = [17.5886  2.3694 -6.9885], b = 43.0722, Train Loss = 4.1080, Test Loss = 5.9540\n",
      "Iteration 23900 : w = [17.6881  2.5726 -6.9068], b = 42.8039, Train Loss = 3.9577, Test Loss = 5.6343\n",
      "Iteration 23950 : w = [17.9195  2.7017 -6.9782], b = 42.5704, Train Loss = 4.1875, Test Loss = 5.6060\n",
      "Iteration 24000 : w = [17.6942  2.3015 -6.9158], b = 42.4524, Train Loss = 3.9416, Test Loss = 5.6213\n",
      "Iteration 24050 : w = [17.8444  2.4977 -6.7805], b = 42.6101, Train Loss = 3.9567, Test Loss = 5.5662\n",
      "Iteration 24100 : w = [17.5231  2.3452 -6.9464], b = 42.2592, Train Loss = 4.1073, Test Loss = 5.7971\n",
      "Iteration 24150 : w = [17.7087  2.3726 -6.8861], b = 42.6212, Train Loss = 3.8928, Test Loss = 5.5918\n",
      "Iteration 24200 : w = [17.8919  2.4492 -6.8778], b = 42.7266, Train Loss = 3.9558, Test Loss = 5.5576\n",
      "Iteration 24250 : w = [17.799   2.3942 -6.7665], b = 42.4490, Train Loss = 3.9454, Test Loss = 5.5670\n",
      "Iteration 24300 : w = [17.7175  2.2482 -6.9681], b = 42.7932, Train Loss = 3.9365, Test Loss = 5.6973\n",
      "Iteration 24350 : w = [17.7168  2.3509 -6.8075], b = 42.1953, Train Loss = 4.0988, Test Loss = 5.6842\n",
      "Iteration 24400 : w = [17.6579  2.409  -6.7784], b = 42.8919, Train Loss = 3.9514, Test Loss = 5.7881\n",
      "Iteration 24450 : w = [17.677   2.4247 -6.9325], b = 42.8710, Train Loss = 3.9498, Test Loss = 5.6978\n",
      "Iteration 24500 : w = [17.7642  2.4701 -6.9667], b = 42.5202, Train Loss = 3.9475, Test Loss = 5.5392\n",
      "Iteration 24550 : w = [17.8349  2.4543 -6.9834], b = 42.7305, Train Loss = 3.9552, Test Loss = 5.5535\n",
      "Iteration 24600 : w = [17.7527  2.1893 -6.7077], b = 42.6580, Train Loss = 3.9181, Test Loss = 5.7206\n",
      "Iteration 24650 : w = [17.8772  2.1726 -6.77  ], b = 42.5662, Train Loss = 3.9013, Test Loss = 5.5970\n",
      "Iteration 24700 : w = [17.6323  1.946  -6.7212], b = 42.8792, Train Loss = 4.1787, Test Loss = 6.1966\n",
      "Iteration 24750 : w = [17.8387  2.2257 -6.8116], b = 42.9401, Train Loss = 3.9691, Test Loss = 5.7710\n",
      "Iteration 24800 : w = [17.814   2.2425 -6.8504], b = 42.5684, Train Loss = 3.8941, Test Loss = 5.5790\n",
      "Iteration 24850 : w = [17.936   2.2441 -6.6177], b = 42.7024, Train Loss = 3.9481, Test Loss = 5.6924\n",
      "Iteration 24900 : w = [18.0731  2.4233 -6.7609], b = 42.7662, Train Loss = 4.0661, Test Loss = 5.6325\n",
      "Iteration 24950 : w = [17.8564  2.475  -7.0058], b = 42.9284, Train Loss = 4.0492, Test Loss = 5.6546\n",
      "Iteration 24999 : w = [17.987   2.4458 -6.9374], b = 42.9486, Train Loss = 4.0962, Test Loss = 5.6684\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Final Train Loss</th>\n",
       "      <th>Final Test Loss</th>\n",
       "      <th>Best Iteration</th>\n",
       "      <th>Train Loss @ Best Test</th>\n",
       "      <th>Best Test Loss</th>\n",
       "      <th>w (params)</th>\n",
       "      <th>b (bias)</th>\n",
       "      <th>Learning Rate @ Best Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0962</td>\n",
       "      <td>5.6684</td>\n",
       "      <td>10003</td>\n",
       "      <td>3.9761</td>\n",
       "      <td>5.4936</td>\n",
       "      <td>[18.112, 2.2216, -6.9445]</td>\n",
       "      <td>42.5587</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Final Train Loss  Final Test Loss  Best Iteration  Train Loss @ Best Test  \\\n",
       "0            4.0962           5.6684           10003                  3.9761   \n",
       "\n",
       "   Best Test Loss                 w (params)  b (bias)  \\\n",
       "0          5.4936  [18.112, 2.2216, -6.9445]   42.5587   \n",
       "\n",
       "   Learning Rate @ Best Test  \n",
       "0                       0.04  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "'upper' is not a valid value for loc; supported values are 'best', 'upper right', 'upper left', 'lower left', 'lower right', 'right', 'center left', 'center right', 'lower center', 'upper center', 'center'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m min_test_loss, min_index, best_w, best_b, df_summary, loss_history_train, loss_history_test, lr_history \u001b[38;5;241m=\u001b[39m exp\u001b[38;5;241m.\u001b[39mtrain_one_lr(initial_lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.04\u001b[39m, iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25000\u001b[39m)\n\u001b[0;32m      6\u001b[0m display(df_summary)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mplot_loss_and_lr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_history_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_history_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_history_test\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_history_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_history\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmin_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_test_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmin_test_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtitle_main\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAdam + SGD : Loss vs Learning Rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubtitle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mInitial_lr = \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitial_lr\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, λ = \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlambda_reg\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, β1 = \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, β2 = \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 35\u001b[0m, in \u001b[0;36mplot_loss_and_lr\u001b[1;34m(loss_history_train, loss_history_test, lr_history, min_index, min_test_loss, title_main, subtitle, figsize, save_path)\u001b[0m\n\u001b[0;32m     23\u001b[0m bbox_props \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(boxstyle \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mround,pad = 0.4\u001b[39m\u001b[38;5;124m'\u001b[39m, fc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m'\u001b[39m, lw \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m, alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.85\u001b[39m)\n\u001b[0;32m     24\u001b[0m ax1\u001b[38;5;241m.\u001b[39mannotate(\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMin : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmin_test_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m @ Iter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmin_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     26\u001b[0m     xy \u001b[38;5;241m=\u001b[39m (min_index, min_test_loss),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m     zorder \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m     34\u001b[0m )\n\u001b[1;32m---> 35\u001b[0m \u001b[43max1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlegend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mupper\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfontsize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframealpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m ax1\u001b[38;5;241m.\u001b[39mgrid(\u001b[38;5;28;01mTrue\u001b[39;00m, linestyle \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdashed\u001b[39m\u001b[38;5;124m'\u001b[39m, linewidth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m, alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m, zorder \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# 副軸\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MignonChen\\anaconda3\\envs\\python3_10\\lib\\site-packages\\matplotlib\\axes\\_axes.py:337\u001b[0m, in \u001b[0;36mAxes.legend\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;03mPlace a legend on the Axes.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m.. plot:: gallery/text_labels_and_annotations/legend.py\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    336\u001b[0m handles, labels, kwargs \u001b[38;5;241m=\u001b[39m mlegend\u001b[38;5;241m.\u001b[39m_parse_legend_args([\u001b[38;5;28mself\u001b[39m], \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 337\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegend_ \u001b[38;5;241m=\u001b[39m mlegend\u001b[38;5;241m.\u001b[39mLegend(\u001b[38;5;28mself\u001b[39m, handles, labels, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegend_\u001b[38;5;241m.\u001b[39m_remove_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remove_legend\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegend_\n",
      "File \u001b[1;32mc:\\Users\\MignonChen\\anaconda3\\envs\\python3_10\\lib\\site-packages\\matplotlib\\legend.py:553\u001b[0m, in \u001b[0;36mLegend.__init__\u001b[1;34m(self, parent, handles, labels, loc, numpoints, markerscale, markerfirst, reverse, scatterpoints, scatteryoffsets, prop, fontsize, labelcolor, borderpad, labelspacing, handlelength, handleheight, handletextpad, borderaxespad, columnspacing, ncols, mode, fancybox, shadow, title, title_fontsize, framealpha, edgecolor, facecolor, bbox_to_anchor, bbox_transform, frameon, handler_map, title_fontproperties, alignment, ncol, draggable)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_legend_box(handles, labels, markerfirst)\n\u001b[0;32m    552\u001b[0m \u001b[38;5;66;03m# Set legend location\u001b[39;00m\n\u001b[1;32m--> 553\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;66;03m# figure out title font properties:\u001b[39;00m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m title_fontsize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m title_fontproperties \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\MignonChen\\anaconda3\\envs\\python3_10\\lib\\site-packages\\matplotlib\\legend.py:671\u001b[0m, in \u001b[0;36mLegend.set_loc\u001b[1;34m(self, loc)\u001b[0m\n\u001b[0;32m    669\u001b[0m             loc \u001b[38;5;241m=\u001b[39m locs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m locs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    670\u001b[0m     \u001b[38;5;66;03m# check that loc is in acceptable strings\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[43m_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m np\u001b[38;5;241m.\u001b[39miterable(loc):\n\u001b[0;32m    673\u001b[0m     \u001b[38;5;66;03m# coerce iterable into tuple\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(loc)\n",
      "File \u001b[1;32mc:\\Users\\MignonChen\\anaconda3\\envs\\python3_10\\lib\\site-packages\\matplotlib\\_api\\__init__.py:184\u001b[0m, in \u001b[0;36mcheck_getitem\u001b[1;34m(mapping, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[v]\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m is not a valid value for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; supported values are \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mrepr\u001b[39m,\u001b[38;5;250m \u001b[39mmapping))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: 'upper' is not a valid value for loc; supported values are 'best', 'upper right', 'upper left', 'lower left', 'lower right', 'right', 'center left', 'center right', 'lower center', 'upper center', 'center'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gdlib.Ex1_42_adam_SGD import Ex1_42_adam_SGD\n",
    "\n",
    "exp = Ex1_42_adam_SGD(master_seed = 42)\n",
    "\n",
    "min_test_loss, min_index, best_w, best_b, df_summary, loss_history_train, loss_history_test, lr_history = exp.train_one_lr(initial_lr = 0.04, iterations = 25000)\n",
    "display(df_summary)\n",
    "\n",
    "plot_loss_and_lr(\n",
    "    loss_history_train = loss_history_train, \n",
    "    loss_history_test = loss_history_test, \n",
    "    lr_history = lr_history, \n",
    "    min_index = min_index, \n",
    "    min_test_loss = min_test_loss, \n",
    "    title_main = 'Adam + SGD : Loss vs Learning Rate', \n",
    "    subtitle = f'Initial_lr = {exp.initial_lr}, λ = {exp.lambda_reg}, β1 = {exp.beta1}, β2 = {exp.beta2}', \n",
    "    save_path = None\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
