{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cead644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868fae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2446f5",
   "metadata": {},
   "source": [
    "### 使用 Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95b5bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 500\n",
    "\n",
    "area = np.random.uniform(10, 50, n_samples)\n",
    "bedrooms = np.random.randint(1, 5, n_samples)\n",
    "age = np.random.uniform(0, 30, n_samples)\n",
    "\n",
    "noise = np.random.normal(0, 2, n_samples)\n",
    "rent = 1.5 * area + 2 * bedrooms - 0.8 * age + 5 + noise\n",
    "\n",
    "X = np.column_stack((area, bedrooms, age))           # 延著欄位方向合併\n",
    "y = rent\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_standardized = scaler.fit_transform(X_train)\n",
    "X_test_standardized = scaler.transform(X_test)\n",
    "\n",
    "w = np.random.rand(3)\n",
    "b = np.random.rand()\n",
    "\n",
    "learning_rate = 0.003\n",
    "iterations = 5000\n",
    "lambda_reg = 0.01\n",
    "batch_size = 64\n",
    "\n",
    "loss_history_train = []\n",
    "loss_history_test = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a22e644",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iterations):\n",
    "    y_pred_train = np.dot(X_train_standardized, w) + b\n",
    "    error_train = y_train - y_pred_train\n",
    "\n",
    "    w_gradient = (-2/len(X_train_standardized)) * np.dot(X_train_standardized.T, error_train) + 2 * lambda_reg * w\n",
    "    b_gradient = (-2/len(X_train_standardized)) * np.sum(error_train)\n",
    "\n",
    "    w -= learning_rate * w_gradient\n",
    "    b -= learning_rate * b_gradient\n",
    "\n",
    "    loss_train = np.mean(error_train ** 2) + lambda_reg * np.sum(w**2)\n",
    "\n",
    "    loss_history_train.append(loss_train)\n",
    "\n",
    "    # 驗證集損失\n",
    "    y_pred_test = np.dot(X_test_standardized, w) + b\n",
    "    error_test = y_test - y_pred_test\n",
    "    loss_test = np.mean(error_test ** 2)\n",
    "    loss_history_test.append(loss_test)\n",
    "\n",
    "    if i % 500 == 0:\n",
    "        print(f\"Iteration {i} : w = {np.round(w, 4)}, b = {b:.4f}, Train_Loss = {loss_train:.4f}, Test_Loss = {loss_test:.4f}\")\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style = 'whitegrid')\n",
    "plt.plot(loss_history_train, label = 'Train Loss', color = 'blue')\n",
    "plt.plot(loss_history_test, label = 'Test Loss', color = 'red', linestyle = 'dashed')\n",
    "plt.title('Loss vs Iterations', fontsize = 14)\n",
    "plt.xlabel('Iterations', fontsize = 12)\n",
    "plt.ylabel('Loss', fontsize = 12)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d25a33",
   "metadata": {},
   "source": [
    "### 使用 Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1874acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.rand(3)\n",
    "b = np.random.rand()\n",
    "\n",
    "learning_rate = 0.003\n",
    "iterations = 10000\n",
    "lambda_reg = 0.01\n",
    "batch_size = 128\n",
    "\n",
    "loss_history_train2 = []\n",
    "loss_history_test2 = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    permutation = np.random.permutation(len(X_train_standardized))\n",
    "    X_shuffled = X_train_standardized[permutation]\n",
    "    y_shuffled = y_train[permutation]\n",
    "\n",
    "    for start_idx in range(0, len(X_train_standardized), batch_size):\n",
    "        end_idx = start_idx + batch_size\n",
    "        X_batch = X_shuffled[start_idx:end_idx]\n",
    "        y_batch = y_shuffled[start_idx:end_idx]\n",
    "\n",
    "        y_pred = np.dot(X_batch ,w) + b\n",
    "        error = y_batch - y_pred\n",
    "\n",
    "        w_gradient = (-2/len(X_batch)) * np.dot(X_batch.T ,error) + 2 * lambda_reg * w\n",
    "        b_gradient = (-2/len(X_batch)) * np.sum(error)\n",
    "\n",
    "        w -= learning_rate * w_gradient\n",
    "        b -= learning_rate * b_gradient\n",
    "\n",
    "    y_pred_train = np.dot(X_train_standardized, w) + b\n",
    "    error_train = y_train - y_pred_train\n",
    "    loss_train2 = np.mean(error_train ** 2) + lambda_reg * np.sum(w ** 2)\n",
    "    loss_history_train2.append(loss_train2)\n",
    "\n",
    "    y_pred_test = np.dot(X_test_standardized, w) + b\n",
    "    error_test = y_test - y_pred_test\n",
    "    loss_test2 = np.mean(error_test ** 2)\n",
    "    loss_history_test2.append(loss_test2)\n",
    "\n",
    "    if i % 500 == 0:\n",
    "        print(f\"iteration {i}: w = {np.round(w, 4)}, b = {b:.4f}, Train_Loss = {loss_train2:.4f}, Test_Loss = {loss_test2:.4f}\")\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style = 'whitegrid')\n",
    "plt.plot(loss_history_train2, label = 'Train Loss', color = 'blue')\n",
    "plt.plot(loss_history_test2, label = 'Test Loss', color = 'red', linestyle = 'dashed')\n",
    "plt.title('Loss vs Iterations', fontsize = 14)\n",
    "plt.xlabel('Iterations', fontsize = 12)\n",
    "plt.ylabel('Loss', fontsize = 12)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e989f662",
   "metadata": {},
   "source": [
    "### 使用三階段策略\n",
    "<li>初期 : Mini-Batch + 固定學習率</li>\n",
    "<li>中期 : Mini-Batch + 學習率衰退</li>\n",
    "<li>後期 : Batch + 繼續衰退學習率</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6c6d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.rand(3)\n",
    "b = np.random.rand()\n",
    "\n",
    "initial_lr = 0.001\n",
    "iterations = 10000\n",
    "lambda_reg = 0.001\n",
    "batch_size = 256\n",
    "\n",
    "early_stage = 3000\n",
    "middle_stage = 7000\n",
    "decay_rate = 0.005\n",
    "\n",
    "\n",
    "loss_history_train3 = []\n",
    "loss_history_test3 = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    # 決定學習率與使用 Batch GD 或 Mini-Batch GD\n",
    "    if i < 3000:\n",
    "        learning_rate = initial_lr\n",
    "        use_batch = False\n",
    "    elif i < 7000:\n",
    "        learning_rate = initial_lr / (1 + decay_rate * (i - early_stage))\n",
    "        use_batch = False\n",
    "    else:\n",
    "        learning_rate = initial_lr / (1 + decay_rate * (middle_stage - early_stage))\n",
    "        use_batch = True\n",
    "\n",
    "    # 資料抽樣\n",
    "    if use_batch:\n",
    "        X_batch = X_train_standardized\n",
    "        y_batch = y_train\n",
    "    else:\n",
    "        permutation = np.random.choice(len(X_train_standardized), size = batch_size)\n",
    "        X_batch = X_train_standardized[permutation]\n",
    "        y_batch = y_train[permutation]\n",
    "    \n",
    "    # 前向預測\n",
    "    y_pred = np.dot(X_batch, w) + b\n",
    "    error = y_batch - y_pred\n",
    "\n",
    "    w_gradient = (-2/len(X_batch)) * np.dot(X_batch.T, error) + 2 * lambda_reg * w\n",
    "    b_gradient = (-2/len(X_batch)) * np.sum(error)\n",
    "\n",
    "    w -= learning_rate * w_gradient\n",
    "    b -= learning_rate * b_gradient\n",
    "\n",
    "    loss_train3 = np.mean(error ** 2) + lambda_reg * np.sum(w ** 2)\n",
    "    loss_history_train3.append(loss_train3)\n",
    "\n",
    "    y_pred_test = np.dot(X_test_standardized, w) + b\n",
    "    error_test = y_test - y_pred_test\n",
    "    loss_test3 = np.mean(error_test ** 2)\n",
    "    loss_history_test3.append(loss_test3)\n",
    "\n",
    "    if i % 500 == 0 or i == (iterations-1):\n",
    "        print(f\"iterations {i} : w = {np.round(w, 4)}, b = {b:.4f}, Train_Loss = {loss_train3:.4f}, Test_Loss = {loss_test3:.4f}\")\n",
    "\n",
    "\n",
    "sns.set(style = 'whitegrid')\n",
    "plt.plot(loss_history_train3, label = 'Train Loss', color = 'blue')\n",
    "plt.plot(loss_history_test3, label = 'Test Loss', color = 'red', linestyle = 'dashed')\n",
    "plt.title('Loss vs Iterations', fontsize = 14)\n",
    "plt.xlabel('Iterations', fontsize = 12)\n",
    "plt.ylabel('Loss', fontsize = 12)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f63c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(loss_history_train3) - np.array(loss_history_test3), color = 'purple')\n",
    "plt.title('Train Loss - Test Loss', fontsize = 14)\n",
    "plt.xlabel('Iterations', fontsize = 12)\n",
    "plt.ylabel('Loss Gap', fontsize = 12)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4a26b0",
   "metadata": {},
   "source": [
    "### 使用 sklearn.linear_model.Ridge 套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0bd7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "ridge_model = Ridge(alpha = 0.001, fit_intercept = True)\n",
    "ridge_model.fit(X_train_standardized, y_train)\n",
    "\n",
    "# 取得參數\n",
    "w_ridge = ridge_model.coef_\n",
    "b_ridge = ridge_model.intercept_\n",
    "\n",
    "# 預測並計算 Loss\n",
    "y_train_pred = ridge_model.predict(X_train_standardized)\n",
    "y_test_pred = ridge_model.predict(X_test_standardized)\n",
    "\n",
    "train_loss_ridge = mean_squared_error(y_train, y_train_pred)\n",
    "test_loss_ridge = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Ridge Regression 結果 : w = {np.round(w_ridge, 4)}, b = {b_ridge:.4f}, Train_Loss = {train_loss_ridge:.4f}, Test_Loss = {test_loss_ridge:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c416debc",
   "metadata": {},
   "source": [
    "### 實驗 Ex1-9 / Ex1-10 / Ex1-11  Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca1f16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "area = np.random.uniform(10, 50, n_samples)\n",
    "expected_bedrooms = np.clip((area/15), 0, 4)\n",
    "bedrooms = np.random.normal(expected_bedrooms, 0.5)\n",
    "bedrooms = np.round(bedrooms).astype(int)\n",
    "bedrooms = np.clip(bedrooms, 0, 4)\n",
    "age = np.random.uniform(0, 30, n_samples)\n",
    "noise = np.random.normal(0, 2, n_samples)\n",
    "rent = 1.5 * area + 2 * bedrooms - 0.8 * age + 5 + noise\n",
    "\n",
    "X = np.column_stack((area, bedrooms, age))\n",
    "y = rent\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "scaler = StandardScaler()\n",
    "X_train_standardized = scaler.fit_transform(X_train)\n",
    "X_test_standardized = scaler.transform(X_test)\n",
    "\n",
    "w = np.random.rand(3)\n",
    "b = np.random.rand()\n",
    "\n",
    "learning_rate = 0.001\n",
    "iterations = 10000\n",
    "lambda_reg = 0.001\n",
    "\n",
    "lr_history = []\n",
    "w_history = []\n",
    "b_history = []\n",
    "loss_history_train_ex1_9 = []\n",
    "loss_history_test_ex1_9 = []\n",
    "\n",
    "loss_history_train_ex1_10 = []\n",
    "loss_history_test_ex1_10 = []\n",
    "\n",
    "loss_history_train_ex1_11 = []\n",
    "loss_history_test_ex1_11 = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    idx = np.random.randint(0, len(X_train_standardized))\n",
    "    x_i = X_train_standardized[idx]\n",
    "    y_i = y_train[idx]\n",
    "\n",
    "    y_pred_i = np.dot(x_i, w) + b\n",
    "    error_i = y_i - y_pred_i\n",
    "\n",
    "    w_gradient = -2 * x_i * error_i + 2 * lambda_reg * w\n",
    "    b_gradient = -2 * error_i\n",
    "\n",
    "    lr_history.append(learning_rate)\n",
    "\n",
    "    w -= learning_rate * w_gradient\n",
    "    b -= learning_rate * b_gradient\n",
    "\n",
    "    w_history.append(w.copy())\n",
    "    b_history.append(b)\n",
    "\n",
    "    y_train_pred = np.dot(X_train_standardized, w) + b\n",
    "    error_train = y_train - y_train_pred\n",
    "    loss_train = np.mean(error_train ** 2) + lambda_reg * np.sum(w ** 2)\n",
    "    loss_history_train_ex1_11.append(loss_train)\n",
    "\n",
    "    y_test_pred = np.dot(X_test_standardized, w) + b\n",
    "    error_test = y_test - y_test_pred\n",
    "    loss_test = np.mean(error_test ** 2)\n",
    "    loss_history_test_ex1_11.append(loss_test)\n",
    "\n",
    "    if i % 50 == 0 or i == (iterations-1):\n",
    "        print(f'iteration {i} : w = {np.round(w, 4)}, b = {b:.4f}, Train Loss = {loss_train:.4f}, Test Loss = {loss_test:.4f}')\n",
    "\n",
    "# 找出最小 Test Loss 及對應的迭代次數\n",
    "min_index = np.argmin(loss_history_test_ex1_11)\n",
    "min_test_loss = loss_history_test_ex1_11[min_index]\n",
    "best_w = w_history[min_index]\n",
    "best_b = b_history[min_index]\n",
    "\n",
    "summary_data = {\n",
    "    'Final Train Loss' : [np.round(loss_history_train_ex1_11[iterations-1], 4)],\n",
    "    'Final Test Loss' : [np.round(loss_history_test_ex1_11[iterations-1], 4)],\n",
    "    'Best Iteration' : [min_index],\n",
    "    'Train Loss @ Best Test' : [np.round(loss_history_train_ex1_11[min_index], 4)],\n",
    "    'Best Test Loss' : [np.round(min_test_loss, 4)],\n",
    "    'w (params)' : [np.round(best_w, 4)],\n",
    "    'b (bias)' : [np.round(best_b, 4)]\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "display(df_summary)\n",
    "\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize = (8, 5.5))\n",
    "fig.text(0.5, 0.893, f'Learning Rate = {learning_rate}', ha = 'center', fontsize = 12, style = 'italic')\n",
    "\n",
    "\n",
    "# 主軸\n",
    "color1 = '#1f77b4'\n",
    "color2 = '#ff7f0e'\n",
    "color3 = '#003366'\n",
    "ax1.set_xlabel('Iterations', fontsize = 14, fontweight = 'bold')\n",
    "ax1.set_ylabel('Loss', color = color1, fontsize = 14, fontweight = 'bold')\n",
    "ax1.plot(loss_history_train_ex1_11, label = 'Train Loss', color = color1, linewidth = 3, alpha = 0.85)\n",
    "ax1.plot(loss_history_test_ex1_11, label = 'Test Loss', color = color2, linestyle = 'dashed', linewidth = 3, alpha = 0.85)\n",
    "ax1.tick_params(axis = 'y', labelcolor = color1)\n",
    "ax1.plot(min_index, min_test_loss, 'o', markersize = 5, label = 'Min Test Loss', color = color3)\n",
    "bbox_props = dict(boxstyle = 'round,pad = 0.4', fc = 'white', lw = 0.8, alpha = 0.85)\n",
    "ax1.annotate(f'Min: {min_test_loss:.4f} @ Iter {min_index}', xy = (min_index, min_test_loss), xytext = (min_index - 4000, min_test_loss + 300), textcoords = 'data', arrowprops = dict(arrowstyle = '-|>', color = color3, lw = 1.2), fontsize = 16, color = color3, bbox = bbox_props)\n",
    "ax1.legend(loc = 'upper right', fontsize = 12, framealpha = 0.9)\n",
    "ax1.set_title('Stochastic Gradient Descent : Loss vs Learning Rate', fontsize = 16, fontweight = 'bold', pad = 24)\n",
    "ax1.grid(True, linestyle = 'dashed', linewidth = 0.5, alpha = 0.4)\n",
    "\n",
    "# 副軸\n",
    "ax2 = ax1.twinx()\n",
    "color3 = '#2ca02c'\n",
    "ax2.set_ylabel('Learning Rate', color = color3, fontsize = 14, fontweight = 'bold')\n",
    "ax2.plot(lr_history, label = 'Learning Rate', color = color3, linestyle = 'dotted', linewidth = 4)\n",
    "ax2.tick_params(axis = 'y', labelcolor = color3)\n",
    "\n",
    "# 自動調整佈局\n",
    "fig.tight_layout()\n",
    "#plt.savefig('ex1_11.png', dpi = 720, bbox_inches = 'tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0de6e2",
   "metadata": {},
   "source": [
    "### 實驗 Ex1-12 / Ex1-13 / Ex1-14  SGD + Inverse Time Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f200415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "area = np.random.uniform(10, 50, n_samples)\n",
    "expected_bedrooms = np.clip((area/15), 0, 4)\n",
    "bedrooms = np.random.normal(expected_bedrooms, 0.5)\n",
    "bedrooms = np.round(bedrooms).astype(int)\n",
    "bedrooms = np.clip(bedrooms, 0, 4)\n",
    "age = np.random.uniform(0, 30, n_samples)\n",
    "noise = np.random.normal(0, 2, n_samples)\n",
    "rent = 1.5 * area + 2 * bedrooms - 0.8 * age + 5 + noise\n",
    "\n",
    "X = np.column_stack((area, bedrooms, age))\n",
    "y = rent\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_standardized = scaler.fit_transform(X_train)\n",
    "X_test_standardized = scaler.transform(X_test)\n",
    "\n",
    "w = np.random.rand(3)\n",
    "b = np.random.rand()\n",
    "\n",
    "initial_lr = 0.001\n",
    "iterations = 25000\n",
    "lambda_reg = 0.001\n",
    "decay_rate = 0.00005\n",
    "\n",
    "w_history = []\n",
    "b_history = []\n",
    "loss_history_train_ex1_12 = []\n",
    "loss_history_test_ex1_12 = []\n",
    "lr_history_inverse_time = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    idx = np.random.randint(0, len(X_train_standardized))\n",
    "    x_i = X_train_standardized[idx]\n",
    "    y_i = y_train[idx]\n",
    "\n",
    "    y_pred_i = np.dot(x_i, w) + b\n",
    "    error_i = y_i - y_pred_i\n",
    "\n",
    "    w_gradient = (-2) * x_i * error_i + 2 * lambda_reg * w\n",
    "    b_gradient = (-2) * error_i\n",
    "\n",
    "    learning_rate = initial_lr / (1 + decay_rate * i)\n",
    "    lr_history_inverse_time.append(learning_rate)\n",
    "\n",
    "    w -= w_gradient * learning_rate\n",
    "    b -= b_gradient * learning_rate\n",
    "\n",
    "    w_history.append(w.copy())\n",
    "    b_history.append(b)\n",
    "\n",
    "    y_pred_train = np.dot(X_train_standardized, w) + b\n",
    "    error_train = y_train - y_pred_train\n",
    "    loss_train = np.mean(error_train ** 2) + lambda_reg * np.sum(w ** 2)\n",
    "    loss_history_train_ex1_12.append(loss_train)\n",
    "\n",
    "    y_pred_test = np.dot(X_test_standardized, w) + b\n",
    "    error_test = y_test - y_pred_test\n",
    "    loss_test = np.mean(error_test ** 2)\n",
    "    loss_history_test_ex1_12.append(loss_test)\n",
    "\n",
    "    if i % 50 == 0 or i == (iterations - 1):\n",
    "        print(f'iteration {i} : w = {np.round(w, 4)}, b = {b:.4f}, Train Loss = {loss_train:.4f}, Test Loss = {loss_test:.4f}')\n",
    "\n",
    "\n",
    "# 找出最小 Test Loss 及對應的迭代次數\n",
    "min_index = np.argmin(loss_history_test_ex1_12)\n",
    "min_test_loss = loss_history_test_ex1_12[min_index]\n",
    "best_w = w_history[min_index]\n",
    "best_b = b_history[min_index]\n",
    "lr_at_test_loss = lr_history_inverse_time[min_index]\n",
    "\n",
    "\n",
    "summary_data = {\n",
    "    'Final Train Loss' : [np.round(loss_history_train_ex1_12[iterations-1], 4)],\n",
    "    'Final Test Loss' : [np.round(loss_history_test_ex1_12[iterations-1], 4)],\n",
    "    'Best Iteration' : [min_index],\n",
    "    'Train Loss @ Best Test' : [np.round(loss_history_train_ex1_12[min_index], 4)],\n",
    "    'Best Test Loss' : [np.round(loss_history_test_ex1_12[min_index], 4)],\n",
    "    'w (params)' : [np.round(best_w, 4)],\n",
    "    'b (bias)' : [np.round(best_b, 4)],\n",
    "    'Learning Rate @ Test Loss' : [np.round(lr_at_test_loss, 4)]\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "\n",
    "display(df_summary)\n",
    "\n",
    "# 設定圖案樣式\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize = (8, 5.5))\n",
    "fig.text(0.5, 0.893, f'SGD with decay rate = {decay_rate}, λ = 0.001', ha = 'center', fontsize = 12, style = 'italic')\n",
    "\n",
    "\n",
    "# 主軸 : 畫 Loss 曲線\n",
    "color1 = '#1f77b4'       # 藍\n",
    "color2 = '#ff7f0e'       # 橘\n",
    "color3 = '#003366'\n",
    "ax1.set_xlabel('Iterations', fontsize = 14, fontweight = 'bold')\n",
    "ax1.set_ylabel('Loss', color = color1, fontsize = 14, fontweight = 'bold')\n",
    "ax1.plot(loss_history_train_ex1_12, label = 'Train Loss', color = color1, linewidth = 3, alpha = 0.85, zorder = 3)\n",
    "ax1.plot(loss_history_test_ex1_12, label = 'Test Loss', color = color2, linestyle = 'dashed', linewidth = 3, alpha = 0.85, zorder = 3)\n",
    "ax1.tick_params(axis = 'y', labelcolor = color1)\n",
    "ax1.plot(min_index, min_test_loss, 'o', markersize = 5, label = 'Min Test Loss', color = color3, zorder = 4)\n",
    "bbox_props = dict(boxstyle = 'round,pad = 0.4', fc = 'white', lw = 0.8, alpha = 0.85)\n",
    "ax1.annotate(f'Min: {min_test_loss:.4f} @ Iter {min_index}', xy = (min_index, min_test_loss), xytext = (min_index - 8000, min_test_loss + 700), textcoords = 'data', arrowprops = dict(arrowstyle = '-|>', color = color3, lw = 1.2, zorder = 4), fontsize = 16, color = color3, bbox = bbox_props, zorder = 4)\n",
    "ax1.legend(loc = 'upper right', fontsize = 12, framealpha = 0.9)\n",
    "ax1.set_title('SGD +  Inverse Time Decay : Loss & Learning Rate', fontsize = 16, fontweight = 'bold', pad = 24)\n",
    "ax1.grid(True, linestyle = 'dashed', linewidth = 0.5, alpha = 0.4, zorder = 0)\n",
    "\n",
    "# 副軸 : 畫 Learning Rate 曲線\n",
    "ax2 = ax1.twinx()\n",
    "color3 = '#2ca02c'      # 綠\n",
    "ax2.set_ylabel('Learning Rate', color = color3, fontsize = 14, fontweight = 'bold')\n",
    "ax2.plot(lr_history_inverse_time, label = 'Learning Rate', color = color3, linestyle = 'dotted', linewidth = 4)\n",
    "ax2.tick_params(axis = 'y', labelcolor = color3)\n",
    "\n",
    "# 自動調整佈局\n",
    "fig.tight_layout()\n",
    "#plt.savefig('ex1_12.png', dpi = 720, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a934d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "w = np.random.rand(3)\n",
    "b = np.random.rand()\n",
    "\n",
    "initial_lr = 0.001\n",
    "# learning_rate = 0.001\n",
    "iterations = 10000\n",
    "lambda_reg = 0.001\n",
    "decay_rate = 0.002\n",
    "\n",
    "loss_history_train4 = []\n",
    "loss_history_test4 = []\n",
    "\n",
    "w_at_2800 = None\n",
    "b_at_2800 = None\n",
    "\n",
    "for i in range(iterations):\n",
    "    # 隨機抽一筆資料\n",
    "    idx = np.random.randint(0, len(X_train_standardized))\n",
    "    x_i = X_train_standardized[idx]\n",
    "    y_i = y_train[idx]\n",
    "\n",
    "    # 預測與誤差\n",
    "    y_pred_i = np.dot(x_i, w) + b\n",
    "    error_i = y_i - y_pred_i\n",
    "\n",
    "    # 計算梯度\n",
    "    w_gradient = -2 * x_i * error_i + 2 * lambda_reg * w\n",
    "    b_gradient = -2 * error_i\n",
    "\n",
    "    # 更新學習率(隨迭代次數遞減)\n",
    "    learning_rate = initial_lr / (1 + decay_rate * i)\n",
    "\n",
    "    # 更新參數\n",
    "    w -= learning_rate * w_gradient\n",
    "    b -= learning_rate * b_gradient\n",
    "\n",
    "    # 每次記錄一次 Loss (可以改成每 10 次記錄一次)\n",
    "    if i % 10 == 0:\n",
    "        y_pred_train = np.dot(X_train_standardized, w) + b\n",
    "        y_pred_test = np.dot(X_test_standardized, w) + b\n",
    "\n",
    "        loss_train4 = np.mean((y_train - y_pred_train) ** 2) + lambda_reg * np.sum(w ** 2)\n",
    "        loss_test4 = np.mean((y_test - y_pred_test) ** 2)\n",
    "\n",
    "        loss_history_train4.append(loss_train4)\n",
    "        loss_history_test4.append(loss_test4)\n",
    "\n",
    "    # 每 500 次印出結果\n",
    "    if i % 500 == 0 or i == (iterations-1):\n",
    "        print(f\"iteration {i} : w = {np.round(w, 4)}, b = {np.round(b, 4)}, Train_Loss = {loss_train4:.4f}, Test_Loss = {loss_test4:.4f}\")\n",
    "\n",
    "    if i == 2800:\n",
    "        w_at_2800 = w.copy()\n",
    "        b_at_2800 = b\n",
    "        print(f\"==> Saved model at iteration {i}: w = {w_at_2800}, b = {b_at_2800}\")\n",
    "    \n",
    "\n",
    "sns.set(style = 'whitegrid')\n",
    "plt.figure(figsize = (8, 6))\n",
    "plt.plot(loss_history_train4, label = 'Train Loss', color = \"blue\")\n",
    "plt.plot(loss_history_test4, label = 'Test Loss', color = 'red', linestyle = 'dashed')\n",
    "plt.title('SGD: Loss vs Iterations', fontsize = 14)\n",
    "plt.xlabel('Iterations', fontsize = 12)\n",
    "plt.ylabel('Loss', fontsize = 12)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4050e5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez('model_at_2800.npz', w = w_at_2800, b = b_at_2800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc13b68b",
   "metadata": {},
   "source": [
    "### 實驗 Ex1-16 / Ex1-17 / Ex1-18  SGD + Moment + Inverse Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "area = np.random.uniform(10, 50, n_samples)\n",
    "expected_bedrooms = np.clip((area/15), 0, 4)\n",
    "bedrooms = np.random.normal(expected_bedrooms, 0.5)\n",
    "bedrooms = np.round(bedrooms).astype(int)\n",
    "bedrooms = np.clip(bedrooms, 0, 4)\n",
    "age = np.random.uniform(0, 30, n_samples)\n",
    "noise = np.random.normal(0, 2, n_samples)\n",
    "rent = 1.5 * area + 2 * bedrooms - 0.8 * age + 5 + noise\n",
    "\n",
    "X = np.column_stack((area, bedrooms, age))\n",
    "y = rent\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_standardized = scaler.fit_transform(X_train)\n",
    "X_test_standardized = scaler.transform(X_test)\n",
    "\n",
    "w = np.random.rand(3)  \n",
    "b = np.random.rand()\n",
    "\n",
    "initial_lr = 0.001\n",
    "iterations = 25000\n",
    "decay_rate = 0.00005\n",
    "lambda_reg = 0.001\n",
    "beta = 0.9\n",
    "v_w = np.zeros_like(w)\n",
    "v_b = 0\n",
    "\n",
    "lr_history_inverse_time = []\n",
    "w_history = []\n",
    "b_history = []\n",
    "loss_history_train_ex1_16 = []\n",
    "loss_history_test_ex1_16 = []\n",
    "\n",
    "loss_history_train_ex1_17 = []\n",
    "loss_history_test_ex1_17 = []\n",
    "\n",
    "loss_history_train_ex1_18 = []\n",
    "loss_history_test_ex1_18 = []\n",
    "\n",
    "print(X_train_standardized.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "for i in range(iterations):\n",
    "    idx = np.random.randint(0, len(X_train_standardized))\n",
    "    x_i = X_train_standardized[idx]\n",
    "    y_i = y_train[idx]\n",
    "\n",
    "    y_pred_i = np.dot(x_i, w) + b\n",
    "    error_i = y_i - y_pred_i\n",
    "\n",
    "    w_gradient = (-2) * x_i * error_i + 2 * lambda_reg * w\n",
    "    b_gradient = (-2) * error_i\n",
    "\n",
    "    learning_rate = initial_lr / (1 + decay_rate * i)\n",
    "    lr_history_inverse_time.append(learning_rate)\n",
    "\n",
    "    v_w = beta * v_w + (1 - beta) * w_gradient\n",
    "    v_b = beta * v_b + (1 - beta) * b_gradient\n",
    "\n",
    "    w -= learning_rate * v_w\n",
    "    b -= learning_rate * v_b\n",
    "\n",
    "    w_history.append(w.copy())\n",
    "    b_history.append(b)\n",
    "\n",
    "    y_pred_train = np.dot(X_train_standardized, w) + b\n",
    "    error_train = y_train - y_pred_train\n",
    "    loss_train = np.mean(error_train ** 2) + lambda_reg * np.sum(w ** 2)\n",
    "    loss_history_train_ex1_16.append(loss_train)\n",
    "\n",
    "    y_pred_test = np.dot(X_test_standardized, w) + b\n",
    "    error_test = y_test - y_pred_test\n",
    "    loss_test = np.mean(error_test ** 2)\n",
    "    loss_history_test_ex1_16.append(loss_test)\n",
    "\n",
    "    if i % 50 == 0 or i == (iterations - 1):\n",
    "        print(f'iterations {i} : w = {np.round(w, 4)}, b = {b:.4f}, Train Loss = {loss_train:.4f}, Test Loss = {loss_test:.4f}')\n",
    "\n",
    "# 找出最小 Test Loss 及對應的迭代次數\n",
    "min_index = np.argmin(loss_history_test_ex1_16)\n",
    "min_test_loss = loss_history_test_ex1_16[min_index]\n",
    "best_w = w_history[min_index]\n",
    "best_b = b_history[min_index]\n",
    "lr_at_test_loss = lr_history_inverse_time[min_index]\n",
    "\n",
    "summary_data = {\n",
    "    'Final Train Loss' : [np.round(loss_history_train_ex1_16[iterations-1], 4)],\n",
    "    'Final Test Loss' : [np.round(loss_history_test_ex1_16[iterations-1], 4)],\n",
    "    'Best Iteration' : [min_index],\n",
    "    'Train Loss @ Best Loss' : [np.round(loss_history_train_ex1_16[min_index], 4)],\n",
    "    'Best Test Loss' : [np.round(loss_history_test_ex1_16[min_index], 4)],\n",
    "    'w (params)' : [np.round(best_w, 4)],\n",
    "    'b (bias)' : [np.round(best_b, 4)],\n",
    "    'Learning Rate @ Test Loss' : [np.round(lr_at_test_loss, 4)]\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "display(df_summary)\n",
    "\n",
    "\n",
    "# 設定圖表樣式\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize = (8, 5.5))\n",
    "fig.text(0.5, 0.893, f'SGD with Momentum | decay rate = {decay_rate}, λ = {lambda_reg}, β= {beta}', ha = 'center', fontsize = 12, style = 'italic')\n",
    "\n",
    "# 主軸 : 畫 Loss 曲線\n",
    "color1 = '#1f77b4'       # 藍\n",
    "color2 = '#ff7f0e'       # 橘\n",
    "color3 = '#003366'\n",
    "ax1.set_xlabel('Iterations', fontsize = 14, fontweight = 'bold')\n",
    "ax1.set_ylabel('Loss', color = color1, fontsize = 14, fontweight = 'bold')\n",
    "ax1.plot(loss_history_train_ex1_16, label = 'Train Loss', color = color1, linewidth = 3, alpha = 0.85, zorder = 3)\n",
    "ax1.plot(loss_history_test_ex1_16, label = 'Test Loss', color = color2, linestyle = 'dashed', linewidth = 3, alpha = 0.85, zorder = 3)\n",
    "ax1.tick_params(axis = 'y', labelcolor = color1)\n",
    "ax1.plot(min_index, min_test_loss, 'o', markersize = 5, label = 'Min Test Loss', color = color3, zorder = 4)\n",
    "bbox_props = dict(boxstyle = 'round,pad = 0.4', fc = 'white', lw = 0.8, alpha = 0.85)\n",
    "ax1.annotate(f'Min: {min_test_loss:.4f} @ Iter {min_index}', xy = (min_index, min_test_loss), xytext = (min_index - 8000, min_test_loss + 700), textcoords = 'data', arrowprops = dict(arrowstyle = '-|>', color = color3, lw = 1.2, zorder = 4), fontsize = 16, color = color3, bbox = bbox_props, zorder = 4)\n",
    "ax1.legend(loc = 'upper right', fontsize = 12, framealpha = 0.9)\n",
    "ax1.set_title('SGD + Momentum + Inverse Time Decay : Loss & Learning Rate', fontsize = 16, fontweight = 'bold', pad = 24)\n",
    "ax1.grid(True, linestyle = 'dashed', linewidth = 0.5, alpha = 0.4, zorder = 0)\n",
    "\n",
    "# 副軸 : 畫 Learning Rate 曲線\n",
    "ax2 = ax1.twinx()\n",
    "color3 = '#2ca02c'      # 綠\n",
    "ax2.set_ylabel('Learning Rate', color = color3, fontsize = 14, fontweight = 'bold')\n",
    "ax2.plot(lr_history_inverse_time, label = 'Learning Rate', color = color3, linestyle = 'dotted', linewidth = 4)\n",
    "ax2.tick_params(axis = 'y', labelcolor = color3)\n",
    "\n",
    "# 自動調整佈局\n",
    "fig.tight_layout()\n",
    "#plt.savefig('ex1_16.png', dpi = 720, bbox_inches = 'tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4873c0ff",
   "metadata": {},
   "source": [
    "### 實驗 Ex1-21 / Ex1-22 / Ex1-23 Exponential Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a634eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "area = np.random.uniform(10, 50, n_samples)\n",
    "expected_bedrooms = np.clip((area/15), 0, 4)\n",
    "bedrooms = np.random.normal(expected_bedrooms, 0.5)\n",
    "bedrooms = np.round(bedrooms).astype(int)\n",
    "bedrooms = np.clip(bedrooms, 0, 4)\n",
    "age = np.random.uniform(0, 30, n_samples)\n",
    "noise = np.random.normal(0, 2, n_samples)\n",
    "rent = 1.5 * area + 2 * bedrooms - 0.8 * age + 5 + noise\n",
    "\n",
    "X = np.column_stack((area, bedrooms, age))\n",
    "y = rent\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "scaler = StandardScaler()\n",
    "X_train_standardized = scaler.fit_transform(X_train)\n",
    "X_test_standardized = scaler.transform(X_test)\n",
    "\n",
    "mean = scaler.mean_\n",
    "scale = scaler.scale_\n",
    "print('mean = ', np.round(mean, 4))\n",
    "print('scale = ', np.round(scale, 4))\n",
    "\n",
    "w = np.random.rand(3)\n",
    "b = np.random.rand()\n",
    "\n",
    "initial_lr = 0.001\n",
    "iterations = 25000\n",
    "lambda_reg = 0.001\n",
    "decay_rate = 0.0001\n",
    "beta = 0.9\n",
    "v_w = np.zeros_like(w)\n",
    "v_b = 0\n",
    "\n",
    "lr_history_exponential = []\n",
    "loss_history_train_ex1_21 = []\n",
    "loss_history_test_ex1_21 = []\n",
    "w_history = []\n",
    "b_history = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    idx = np.random.randint(0, len(X_train_standardized))\n",
    "    x_i = X_train_standardized[idx]\n",
    "    y_i = y_train[idx]\n",
    "\n",
    "    y_pred_i = np.dot(x_i, w) + b\n",
    "    error_i = y_i - y_pred_i\n",
    "\n",
    "    w_gradient = (-2) * x_i * error_i + 2 * lambda_reg * w\n",
    "    b_gradient = (-2) * error_i\n",
    "\n",
    "    v_w = beta * v_w + (1 - beta) * w_gradient\n",
    "    v_b = beta * v_b + (1 - beta) * b_gradient\n",
    "\n",
    "    learning_rate = initial_lr * math.exp(-decay_rate * i)\n",
    "    lr_history_exponential.append(learning_rate)\n",
    "\n",
    "    w -= learning_rate * v_w\n",
    "    b -= learning_rate * v_b\n",
    "\n",
    "    w_history.append(w.copy())\n",
    "    b_history.append(b)\n",
    "\n",
    "    y_pred_train = np.dot(X_train_standardized, w) + b\n",
    "    error_train = y_train - y_pred_train\n",
    "    loss_train = np.mean(error_train ** 2) + lambda_reg * np.sum(w ** 2)\n",
    "    loss_history_train_ex1_21.append(loss_train)\n",
    "\n",
    "    y_pred_test = np.dot(X_test_standardized, w) + b\n",
    "    error_test = y_test - y_pred_test\n",
    "    loss_test = np.mean(error_test ** 2)\n",
    "    loss_history_test_ex1_21.append(loss_test)\n",
    "\n",
    "    if i % 50 == 0 or i == (iterations-1):\n",
    "        print(f'iteration {i} : w = {np.round(w, 4)}, b = {b:.4f}, Train_Loss = {loss_train:.4f}, Test_Loss = {loss_test:.4f}')\n",
    "\n",
    "# 找出最小 Test Loss 及對應的迭代次數\n",
    "min_index = np.argmin(loss_history_test_ex1_21)\n",
    "min_test_loss = loss_history_test_ex1_21[min_index]\n",
    "best_w = w_history[min_index]\n",
    "best_b = b_history[min_index]\n",
    "lr_at_test_loss = lr_history_exponential[min_index]\n",
    "\n",
    "summary_data = {\n",
    "    'Final Train Loss' : [np.round(loss_history_train_ex1_21[iterations-1], 4)],\n",
    "    'Final Test Loss' : [np.round(loss_history_test_ex1_21[iterations-1], 4)],\n",
    "    'Best Iteration' : [min_index],\n",
    "    'Train Loss @ Best Test' : [np.round(loss_history_train_ex1_21[min_index], 4)],\n",
    "    'Best Test Loss' : [np.round(min_test_loss, 4)],\n",
    "    'w (params)' : [np.round(best_w, 4)],\n",
    "    'b (bias)' : [np.round(best_b, 4)], \n",
    "    'Learning Rate @ Best test' : [np.round(lr_at_test_loss, 4)]\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "display(df_summary)\n",
    "\n",
    "# 設定圖表樣式\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize = (8, 5.5))\n",
    "fig.text(0.5, 0.893, f'SGD with Momentum | decay rate = {decay_rate}, λ = {lambda_reg}, β= {beta}', ha = 'center', fontsize = 12, style = 'italic')\n",
    "\n",
    "# 主軸 : 畫 Loss 曲線\n",
    "color1 = '#1f77bc'\n",
    "color2 = '#ff7f0e'\n",
    "color3 = '#003366'\n",
    "ax1.set_xlabel('Iterations', fontsize = 14, fontweight = 'bold')\n",
    "ax1.set_ylabel('Loss', color = color1, fontsize = 14, fontweight = 'bold')\n",
    "ax1.plot(loss_history_train_ex1_21, label = 'Train Loss', color = color1, linewidth = 3, alpha = 0.85, zorder = 3)\n",
    "ax1.plot(loss_history_test_ex1_21, label = 'Test Loss', color = color2, linestyle = 'dashed', linewidth = 3, alpha = 0.85, zorder = 3)\n",
    "ax1.tick_params(axis = 'y', labelcolor = color1)\n",
    "ax1.plot(min_index, min_test_loss, 'o', markersize = 5, label = 'Min Test Loss', color = color3, zorder = 4)\n",
    "bbox_props = dict(boxstyle = 'round,pad = 0.4', fc = 'white', lw = 0.8, alpha = 0.85)\n",
    "ax1.annotate(f'Min: {min_test_loss:.4f} @ Iter {min_index}', xy = (min_index, min_test_loss), xytext = (min_index - 10000, min_test_loss + 500), textcoords = 'data', arrowprops = dict(arrowstyle = '-|>', color = color3, lw = 1.2, zorder = 4), fontsize = 16, color = color3, bbox = bbox_props, zorder = 4)\n",
    "ax1.legend(loc = 'upper right', fontsize = 12, framealpha = 0.9)\n",
    "ax1.set_title('SGD + Momentum + Exponential Decay : Loss & Learning Rate', fontsize = 16, fontweight = 'bold', pad = 24)\n",
    "ax1.grid(True, linestyle = 'dashed', linewidth = 0.5, alpha = 0.4, zorder = 0)\n",
    "\n",
    "# 副軸 : 畫 Learning Rate 曲線\n",
    "ax2 = ax1.twinx()\n",
    "color3 = '#2ca02c'\n",
    "ax2.set_ylabel('Learning Rate', color = color3, fontsize = 14, fontweight = 'bold')\n",
    "ax2.plot(lr_history_exponential, label = 'Learning Rate', color = color3, linestyle = 'dotted', linewidth = 4)\n",
    "ax2.tick_params(axis = 'y', labelcolor = color3)\n",
    "\n",
    "# 自動調整佈局\n",
    "fig.tight_layout()\n",
    "#fig.savefig('ex1_22.png', dpi = 720, bbox_inches = 'tight')\n",
    "fig.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6f7e3c",
   "metadata": {},
   "source": [
    "### 實驗 Ex1-24 / Ex1-25 / Ex1-26 Step Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd7d7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "area = np.random.uniform(10, 50, n_samples)\n",
    "expected_bedrooms = np.clip((area/15), 0, 4)\n",
    "bedrooms = np.random.normal(expected_bedrooms, 0.5)\n",
    "bedrooms = np.round(bedrooms).astype(int)\n",
    "bedrooms = np.clip(bedrooms, 0, 4)\n",
    "age = np.random.uniform(0, 30, n_samples)\n",
    "noise = np.random.normal(0, 2, n_samples)\n",
    "rent = 1.5 * area + 2 * bedrooms - 0.8 * age + 5 + noise\n",
    "\n",
    "X = np.column_stack((area, bedrooms, age))\n",
    "y = rent\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "scaler = StandardScaler()\n",
    "X_train_standardized = scaler.fit_transform(X_train)\n",
    "X_test_standardized = scaler.transform(X_test)\n",
    "\n",
    "w = np.random.rand(3)\n",
    "b = np.random.rand()\n",
    "\n",
    "initial_lr = 0.05\n",
    "iterations = 25000\n",
    "lambda_reg = 0.001\n",
    "drop_rate = 0.5\n",
    "step_size = 2500\n",
    "beta = 0.9\n",
    "v_w = np.zeros_like(w)\n",
    "v_b = 0\n",
    "\n",
    "lr_history_step = []\n",
    "loss_history_train_ex1_24 = []\n",
    "loss_history_test_ex1_24 = []\n",
    "w_history = []\n",
    "b_history = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    idx = np.random.randint(0, len(X_train_standardized))\n",
    "    x_i = X_train_standardized[idx]\n",
    "    y_i = y_train[idx]\n",
    "\n",
    "    y_pred_i = np.dot(x_i, w) + b\n",
    "    error_i = y_i - y_pred_i\n",
    "\n",
    "    w_gradient = (-2) * x_i * error_i + 2 * lambda_reg * w\n",
    "    b_gradient = (-2) * error_i\n",
    "\n",
    "    v_w = beta * v_w + (1-beta) * w_gradient\n",
    "    v_b = beta * v_b + (1-beta) * b_gradient\n",
    "\n",
    "    learning_rate = initial_lr * (drop_rate ** (i//step_size))\n",
    "    lr_history_step.append(learning_rate)\n",
    "\n",
    "    w -= learning_rate * v_w\n",
    "    b -= learning_rate * v_b\n",
    "\n",
    "    w_history.append(w.copy())\n",
    "    b_history.append(b)\n",
    "\n",
    "    y_pred_train = np.dot(X_train_standardized, w) + b\n",
    "    error_train = y_train - y_pred_train\n",
    "    loss_train = np.mean(error_train ** 2) + lambda_reg * np.sum(w**2)\n",
    "    loss_history_train_ex1_24.append(loss_train)\n",
    "\n",
    "    y_pred_test = np.dot(X_test_standardized, w) + b\n",
    "    error_test = y_test - y_pred_test\n",
    "    loss_test = np.mean(error_test ** 2)\n",
    "    loss_history_test_ex1_24.append(loss_test)\n",
    "\n",
    "    if i % 50 == 0 or i == (iterations-1):\n",
    "        print(f'iteration {i}: w = {np.round(w,4)}, b = {b:.4f}, Train Loss = {loss_train:.4f}, Test Loss = {loss_test:.4f}')\n",
    "\n",
    "min_index = np.argmin(loss_history_test_ex1_24)\n",
    "min_test_loss = loss_history_test_ex1_24[min_index]   \n",
    "best_w = w_history[min_index]\n",
    "best_b = b_history[min_index]\n",
    "lr_at_test_loss = lr_history_step[min_index]\n",
    "\n",
    "summary_data = {\n",
    "    'Final Train Loss' : [np.round(loss_history_train_ex1_24[iterations-1], 4)],\n",
    "    'Final Test Loss' : [np.round(loss_history_test_ex1_24[iterations-1], 4)],\n",
    "    'Best iteration' : [min_index],\n",
    "    'Train Loss @ Best Test' : [np.round(loss_history_train_ex1_24[min_index],4)],\n",
    "    'Best Test Loss' : [np.round(min_test_loss, 4)],\n",
    "    'w (params)' : [np.round(best_w, 4)],\n",
    "    'b (bias)' : [np.round(best_b, 4)],\n",
    "    'Learning Rate @ Best Test' : [np.round(lr_at_test_loss, 4)]\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "display(df_summary)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "fig, ax1 = plt.subplots(figsize = (8, 5.5))\n",
    "fig.text(0.5, 0.893, f'SGD with Momentum | drop rate = {drop_rate}, step size = {step_size}, λ = {lambda_reg}, β= {beta}', ha = 'center', fontsize = 12, style = 'italic')\n",
    "\n",
    "# 主軸 : 畫 Loss 曲線\n",
    "color1 = '#1f77bc'\n",
    "color2 = '#ff7f0e'\n",
    "color3 = '#003366'\n",
    "ax1.set_xlabel('Iterations', fontsize = 14, fontweight = 'bold')\n",
    "ax1.set_ylabel('Loss', color = color1, fontsize = 14, fontweight = 'bold')\n",
    "ax1.plot(loss_history_train_ex1_24, label = 'Train Loss', color = color1, linewidth = 3, alpha = 0.85, zorder = 3)\n",
    "ax1.plot(loss_history_test_ex1_24, label = 'Test Loss', color = color2, linestyle = 'dashed', linewidth = 3, alpha = 0.85, zorder = 3)\n",
    "ax1.tick_params(axis = 'y', labelcolor = color1)\n",
    "ax1.plot(min_index, min_test_loss, 'o', markersize = 5, label = 'Min Test Loss', color = color3, zorder = 4)\n",
    "bbox_props = dict(boxstyle = 'round,pad = 0.4', fc = 'white', lw = 0.8, alpha = 0.85)\n",
    "ax1.annotate(f'Min: {min_test_loss:.4f} @ Iter {min_index}', xy = (min_index, min_test_loss), xytext = (min_index + 4000, min_test_loss + 600), textcoords = 'data', arrowprops = dict(arrowstyle = '-|>', color = color3, lw = 1.2, zorder = 4), fontsize = 16, color = color3, bbox = bbox_props, zorder = 4)\n",
    "ax1.legend(loc = 'upper right', fontsize = 12, framealpha = 0.9)\n",
    "ax1.set_title('SGD + Momentum + Step Decay : Loss vs Learning Rate', fontsize = 16, fontweight = 'bold', pad = 24)\n",
    "ax1.grid(True, linestyle = 'dashed', linewidth = 0.5, alpha = 0.4, zorder = 0)\n",
    "\n",
    "# 副軸 : 畫 Learning Rate 曲線\n",
    "ax2 = ax1.twinx()\n",
    "color3 = '#2ca02c'\n",
    "ax2.set_ylabel('Learning Rate', color = color3, fontsize = 14, fontweight = 'bold')\n",
    "ax2.plot(lr_history_step, label = 'Learning Rate', color = color3, linestyle = 'dotted', linewidth = 4)\n",
    "ax2.tick_params(axis = 'y', labelcolor = color3)\n",
    "\n",
    "# 自動調整佈局\n",
    "fig.tight_layout()\n",
    "#fig.savefig('ex1_24.png', dpi = 720, bbox_inches = 'tight')\n",
    "fig.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0edb4b2",
   "metadata": {},
   "source": [
    "### 實驗 Ex1-27 / Ex1-28 / Ex1-29 Polynomial Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59f6615",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "area = np.random.uniform(10, 50, n_samples)\n",
    "expected_bedrooms = np.clip((area/15), 0, 4)\n",
    "bedrooms = np.random.normal(expected_bedrooms, 0.5)\n",
    "bedrooms = np.round(bedrooms).astype(int)\n",
    "bedrooms = np.clip(bedrooms, 0, 4)\n",
    "age = np.random.uniform(0, 30, n_samples)\n",
    "noise = np.random.normal(0, 2, n_samples)\n",
    "rent = 1.5 * area + 2 * bedrooms - 0.8 * age + 5 + noise\n",
    "\n",
    "X = np.column_stack((area, bedrooms, age))\n",
    "y = rent\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "scaler = StandardScaler()\n",
    "X_train_standardized = scaler.fit_transform(X_train)\n",
    "X_test_standardized = scaler.transform(X_test)\n",
    "\n",
    "w = np.random.rand(3)\n",
    "b = np.random.rand()\n",
    "\n",
    "initial_lr = 0.001\n",
    "iterations = 25000\n",
    "lambda_reg = 0.001\n",
    "beta = 0.9\n",
    "v_w = np.zeros_like(w)\n",
    "v_b = 0\n",
    "power = 2\n",
    "\n",
    "lr_history_polynomial = []\n",
    "loss_history_train_ex1_27 = []\n",
    "loss_history_test_ex1_27 = []\n",
    "w_history = []\n",
    "b_history = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    idx = np.random.randint(0, len(X_train_standardized))\n",
    "    x_i = X_train_standardized[idx]\n",
    "    y_i = y_train[idx]\n",
    "\n",
    "    y_pred_i = np.dot(x_i, w) + b\n",
    "    error_i = y_i - y_pred_i\n",
    "\n",
    "    w_gradient = (-2) * x_i * error_i + 2 * lambda_reg * w\n",
    "    b_gradient = (-2) * error_i\n",
    "\n",
    "    v_w = beta * v_w + (1 - beta) * w_gradient\n",
    "    v_b = beta * v_b + (1 - beta) * b_gradient\n",
    "\n",
    "    learning_rate = initial_lr * (1 - (i / iterations)) ** power\n",
    "    lr_history_polynomial.append(learning_rate)\n",
    "\n",
    "    w -= learning_rate * v_w\n",
    "    b -= learning_rate * v_b\n",
    "\n",
    "    w_history.append(w.copy())\n",
    "    b_history.append(b)\n",
    "\n",
    "    y_train_pred = np.dot(X_train_standardized, w) + b\n",
    "    error_train = y_train - y_train_pred\n",
    "    loss_train = np.mean(error_train ** 2) + lambda_reg * np.sum(w ** 2)\n",
    "    loss_history_train_ex1_27.append(loss_train)\n",
    "\n",
    "    y_test_pred = np.dot(X_test_standardized, w) + b\n",
    "    error_test = y_test - y_test_pred\n",
    "    loss_test = np.mean(error_test ** 2)\n",
    "    loss_history_test_ex1_27.append(loss_test)\n",
    "\n",
    "    if i % 5 == 0 or i == (iterations-1):\n",
    "        print(f'iteration {i}: w = {np.round(w, 4)}, b = {b:.4f}, Train Loss = {loss_train:.4f}, Test Loss = {loss_test:.4f}')\n",
    "\n",
    "min_index = np.argmin(loss_history_test_ex1_27)\n",
    "min_test_loss = loss_history_test_ex1_27[min_index]\n",
    "best_w = w_history[min_index]\n",
    "best_b = b_history[min_index]\n",
    "lr_at_test_loss = lr_history_polynomial[min_index]\n",
    "\n",
    "summary_data = {\n",
    "    'Final Train' : [np.round(loss_history_train_ex1_27[iterations-1], 4)],\n",
    "    'Final Test' : [np.round(loss_history_test_ex1_27[iterations-1], 4)],\n",
    "    'Best iteration' : [min_index],\n",
    "    'Train Loss @ Best Test' : [np.round(loss_history_train_ex1_27[min_index], 4)],\n",
    "    'Best Test Loss' : [np.round(min_test_loss, 4)],\n",
    "    'w (params)' : [np.round(best_w, 4)],\n",
    "    'b (bias)' : [np.round(best_b, 4)],\n",
    "    'Learning Rate @ Best Test' : [np.round(lr_at_test_loss, 4)]\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "display(df_summary)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "fig, ax1 = plt.subplots(figsize = (8, 5.5))\n",
    "fig.text(0.5, 0.893, f'SGD with Momentum | power = {power}, λ = {lambda_reg}, β= {beta}', ha = 'center', fontsize = 12, style = 'italic')\n",
    "\n",
    "# 主軸\n",
    "color1 = '#1f77bc'\n",
    "color2 = '#ff7f0e'\n",
    "color3 = '#003366'\n",
    "ax1.set_xlabel('Iterations', fontsize = 14, fontweight = 'bold')\n",
    "ax1.set_ylabel('Loss', color = color1, fontsize = 14, fontweight = 'bold')\n",
    "ax1.plot(loss_history_train_ex1_27, label = 'Train Loss', color = color1, linewidth = 3, alpha = 0.85, zorder = 3)\n",
    "ax1.plot(loss_history_test_ex1_27, label = 'Test Loss', color = color2, linestyle = 'dashed', linewidth = 3, alpha = 0.85, zorder = 3)\n",
    "ax1.tick_params(axis = 'y', labelcolor = color1)\n",
    "ax1.plot(min_index, min_test_loss, 'o', markersize = 5, label = 'Min Test Loss', color = color3, zorder = 4)\n",
    "bbox_props = dict(boxstyle = 'round,pad = 0.4', fc = 'white', lw = 0.8, alpha = 0.85)\n",
    "ax1.annotate(f'Min: {min_test_loss:.4f} @ Iter {min_index}', xy = (min_index, min_test_loss), xytext = (min_index - 11000, min_test_loss + 600), textcoords = 'data', arrowprops = dict(arrowstyle = '-|>', color = color3, lw = 1.2, zorder = 4), fontsize = 16, color = color3, bbox = bbox_props, zorder = 4)\n",
    "ax1.legend(loc = 'upper right', fontsize = 12, framealpha = 0.9)\n",
    "ax1.set_title('SGD + Momentum + Polynomial Decay : Loss vs Learning Rate', fontsize = 16, fontweight = 'bold', pad = 24)\n",
    "ax1.grid(True, linestyle = 'dashed', linewidth = 0.5, alpha = 0.4, zorder = 0)\n",
    "\n",
    "# 副軸 : 畫 Learning Rate 曲線\n",
    "color3 = '#2ca02c'\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Learning Rate', color = color3, fontsize = 14, fontweight = 'bold')\n",
    "ax2.plot(lr_history_polynomial, label = 'Learning Rate', color = color3, linestyle = 'dotted', linewidth = 4)\n",
    "ax2.tick_params(axis = 'y', labelcolor = color3)\n",
    "\n",
    "# 自動調整佈局\n",
    "fig.tight_layout()\n",
    "#fig.savefig('ex1_29.png', dpi = 720, bbox_inches = 'tight')\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8b5f42",
   "metadata": {},
   "source": [
    "### 實驗 Ex1-30 / Ex1-31 / Ex1-32 SGD + Momentum + Cosine Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2e13e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "area = np.random.uniform(10, 50, n_samples)\n",
    "expected_bedrooms = np.clip((area/15), 0, 4)\n",
    "bedrooms = np.random.normal(expected_bedrooms, 0.5)\n",
    "bedrooms = np.round(bedrooms).astype(int)\n",
    "bedrooms = np.clip(bedrooms, 0, 4)\n",
    "age = np.random.uniform(0, 30, n_samples)\n",
    "noise = np.random.normal(0, 2, n_samples)\n",
    "rent = 1.5 * area + 2 * bedrooms - 0.8 * age + 5 + noise\n",
    "\n",
    "X = np.column_stack((area, bedrooms, age))\n",
    "y = rent\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "scaler = StandardScaler()\n",
    "X_train_standardized = scaler.fit_transform(X_train)\n",
    "X_test_standardized = scaler.transform(X_test)\n",
    "\n",
    "w = np.random.rand(3)\n",
    "b = np.random.rand()\n",
    "\n",
    "initial_lr = 0.001\n",
    "iterations = 25000\n",
    "lambda_reg = 0.001\n",
    "min_lr = 0.0005\n",
    "beta = 0.9\n",
    "v_w = np.zeros_like(w)\n",
    "v_b = 0\n",
    "\n",
    "lr_history_cosine_annealing = []\n",
    "loss_history_train_ex1_30 = []\n",
    "loss_history_test_ex1_30 = []\n",
    "w_history = []\n",
    "b_history = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    idx = np.random.randint(0, len(X_train_standardized))\n",
    "    x_i = X_train_standardized[idx]\n",
    "    y_i = y_train[idx]\n",
    "\n",
    "    y_pred_i = np.dot(x_i, w) + b\n",
    "    error_i = y_i - y_pred_i\n",
    "\n",
    "    w_gradient = (-2) * x_i * error_i + 2 * lambda_reg * w\n",
    "    b_gradient = (-2) * error_i\n",
    "\n",
    "    v_w = beta * v_w + (1 - beta) * w_gradient\n",
    "    v_b = beta * v_b + (1 - beta) * b_gradient\n",
    "\n",
    "    learning_rate = min_lr + (1/2) * (initial_lr - min_lr) * (1 + math.cos(i / iterations * math.pi))\n",
    "    lr_history_cosine_annealing.append(learning_rate)\n",
    "\n",
    "    w -= learning_rate * v_w\n",
    "    b -= learning_rate * v_b\n",
    "\n",
    "    w_history.append(w.copy())\n",
    "    b_history.append(b)\n",
    "\n",
    "    y_pred_train = np.dot(X_train_standardized, w) + b\n",
    "    error_train = y_train - y_pred_train\n",
    "    loss_train = np.mean(error_train ** 2) + lambda_reg * np.sum(w ** 2)\n",
    "    loss_history_train_ex1_30.append(loss_train)\n",
    "\n",
    "    y_pred_test = np.dot(X_test_standardized, w) + b\n",
    "    error_test = y_test - y_pred_test\n",
    "    loss_test = np.mean(error_test ** 2)\n",
    "    loss_history_test_ex1_30.append(loss_test)\n",
    "\n",
    "    if i % 50 == 0 or i == (iterations-1):\n",
    "        print(f'iteration {i}: w = {np.round(w,4)}, b = {b:.4f}, Train Loss = {loss_train:.4f}, Test Loss = {loss_test:.4f}')\n",
    "\n",
    "min_index = np.argmin(loss_history_test_ex1_30)\n",
    "min_test_loss = loss_history_test_ex1_30[min_index]\n",
    "best_w = w_history[min_index]\n",
    "best_b = b_history[min_index]\n",
    "lr_at_test_loss = lr_history_cosine_annealing[min_index]\n",
    "\n",
    "summary_data = {\n",
    "    'Final Train Loss' : [np.round(loss_history_train_ex1_30[iterations-1], 4)],\n",
    "    'Final Test Loss' : [np.round(loss_history_test_ex1_30[iterations-1], 4)],\n",
    "    'Best iteration' : [min_index],\n",
    "    'Train Loss @ Best Test' : [np.round(loss_history_train_ex1_30[min_index], 4)],\n",
    "    'Best Test Loss' : [np.round(loss_history_test_ex1_30[min_index], 4)],\n",
    "    'w (params)' : [np.round(best_w, 4)],\n",
    "    'b (bias)' : [np.round(best_b, 4)],\n",
    "    'Learning Rate @ Best Test' : [np.round(lr_at_test_loss, 4)]\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "display(df_summary)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "fig, ax1 = plt.subplots(figsize = (8, 5.5))\n",
    "fig.text(0.5, 0.893, f'SGD with Momentum | Max_lr = {initial_lr}, Min_lr = {min_lr}, λ = {lambda_reg}, β= {beta}', ha = 'center', fontsize = 12, style = 'italic')\n",
    "\n",
    "# 主軸\n",
    "color1 = '#1f77bc'\n",
    "color2 = '#ff7f0e'\n",
    "color3 = '#003366'\n",
    "ax1.set_xlabel('Iterations', fontsize = 14, fontweight = 'bold')\n",
    "ax1.set_ylabel('Loss', color = color1, fontsize = 14, fontweight = 'bold')\n",
    "ax1.plot(loss_history_train_ex1_30, label = 'Train Loss', color = color1, linewidth = 3, alpha = 0.85, zorder = 3)\n",
    "ax1.plot(loss_history_test_ex1_30, label = 'Test Loss', color = color2, linewidth = 3, linestyle = 'dashed', alpha = 0.85, zorder = 3)\n",
    "ax1.tick_params(axis = 'y', labelcolor = color1)\n",
    "ax1.plot(min_index, min_test_loss, 'o', markersize = 5, label = 'Min Test Loss', color = color3, zorder = 4)\n",
    "bbox_props = dict(boxstyle = 'round,pad = 0.4', fc = 'white', lw = 0.8, alpha = 0.85)\n",
    "ax1.annotate(f'Min: {min_test_loss:.4f} @ Iter {min_index}', xy = (min_index, min_test_loss), xytext = (min_index - 8000, min_test_loss + 1000), textcoords = 'data', arrowprops = dict(arrowstyle = '-|>', color = color3, lw = 1.2, zorder = 4), fontsize = 16, color = color3, bbox = bbox_props, zorder = 4)\n",
    "ax1.legend(loc = 'upper right', fontsize = 12, framealpha = 0.9)\n",
    "ax1.set_title('SGD + Momentum + Cosine Annealing Decay : Loss vs Learning Rate', fontsize = 16, fontweight = 'bold', pad = 24)\n",
    "ax1.grid(True, linestyle = 'dashed', linewidth = 0.5, alpha = 0.4, zorder = 0)\n",
    "\n",
    "# 副軸 : 畫 Learning Rate 曲線\n",
    "color3 = '#2ca02c'\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Learning Rate', color = color3, fontsize = 14, fontweight = 'bold')\n",
    "ax2.plot(lr_history_cosine_annealing, label = 'Learning Rate', color = color3, linestyle = 'dotted', linewidth = 4)\n",
    "ax2.tick_params(axis = 'y', labelcolor = color3)\n",
    "\n",
    "# 自動調整佈局\n",
    "fig.tight_layout()\n",
    "#fig.savefig('ex1_32.png', dpi = 720, bbox_inches = 'tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eb5b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "w = np.random.rand(3)        # 均勻分布, 浮點數, 範圍 [0,1)    \n",
    "b = np.random.rand()\n",
    "\n",
    "learning_rate = 0.001\n",
    "iterations = 10000\n",
    "lambda_reg = 0.001\n",
    "beta = 0.9\n",
    "v_w = np.zeros_like(w)\n",
    "v_b = 0\n",
    "\n",
    "loss_history_train5 = []\n",
    "loss_history_test5 = []\n",
    "\n",
    "print(X_train_standardized.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "for i in range(iterations):\n",
    "    idx = np.random.randint(0, len(X_train_standardized))\n",
    "    x_i = X_train_standardized[idx]\n",
    "    y_i = y_train[idx]\n",
    "\n",
    "    y_pred_i = np.dot(x_i, w) + b\n",
    "    error_i = y_i - y_pred_i\n",
    "\n",
    "    w_gradient = (-2) * x_i * error_i + 2 * lambda_reg * w\n",
    "    b_gradient = (-2) *  error_i\n",
    "\n",
    "    v_w = beta * v_w + (1 - beta) * w_gradient\n",
    "    v_b = beta * v_b + (1 - beta) * b_gradient\n",
    "\n",
    "    w -= learning_rate * v_w\n",
    "    b -= learning_rate * v_b\n",
    "\n",
    "    y_pred_train = np.dot(X_train_standardized, w) + b\n",
    "    error_train = y_train - y_pred_train\n",
    "    loss_train = np.mean(error_train ** 2) + lambda_reg * np.sum(w ** 2)\n",
    "    loss_history_train5.append(loss_train)\n",
    "\n",
    "    y_pred_test = np.dot(X_test_standardized, w) + b\n",
    "    error_test = y_test - y_pred_test\n",
    "    loss_test = np.mean(error_test ** 2)\n",
    "    loss_history_test5.append(loss_test)\n",
    "\n",
    "    if i % 500 == 0 or i == (iterations - 1):\n",
    "        print(f'iterations {i} : w = {np.round(w, 4)}, b = {b:.4f}, Train_Loss = {loss_train:.4f}, Test_Loss = {loss_test:.4f}')\n",
    "\n",
    "sns.set(style = 'whitegrid')\n",
    "plt.figure(figsize = (8, 6))\n",
    "plt.plot(loss_history_train5, label = 'Train Loss', color = 'blue')\n",
    "plt.plot(loss_history_test5, label = 'Test Loss', color = 'red', linestyle = 'dashed')\n",
    "plt.title('SGD + Momentum Loss vs Iterations', fontsize = 14)\n",
    "plt.xlabel('Iterations', fontsize = 12)\n",
    "plt.ylabel('Loss', fontsize = 12)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fd0b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "w = np.random.rand(3)\n",
    "b = np.random.rand()\n",
    "\n",
    "initial_lr = 0.003\n",
    "iterations = 10000\n",
    "lambda_reg = 0.01\n",
    "decay_rate = 0.001\n",
    "beta = 0.9\n",
    "v_w = np.zeros_like(w)\n",
    "v_b = 0\n",
    "\n",
    "loss_history_train6 = []\n",
    "loss_history_test6 = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    idx = np.random.randint(0, X_train_standardized.shape[0])\n",
    "    x_i = X_train_standardized[idx]\n",
    "    y_i = y_train[idx]\n",
    "\n",
    "    y_pred_i = np.dot(x_i, w) + b\n",
    "    error_i = y_i - y_pred_i\n",
    "\n",
    "    w_gradient = (-2) * x_i * error_i + 2 * lambda_reg * w\n",
    "    b_gradient = (-2) * error_i\n",
    "\n",
    "    learning_rate = initial_lr / (1 + decay_rate * i)   # Inverse Time Decay (反時間衰減)\n",
    "\n",
    "    v_w = beta * v_w + (1 - beta) * w_gradient\n",
    "    v_b = beta * v_b + (1 - beta) * b_gradient\n",
    "\n",
    "    w -= learning_rate * v_w\n",
    "    b -= learning_rate * v_b\n",
    "\n",
    "    y_pred_train = np.dot(X_train_standardized, w) + b\n",
    "    error_train = y_train - y_pred_train\n",
    "    loss_train = np.mean(error_train ** 2) + lambda_reg * np.sum(w ** 2)\n",
    "    loss_history_train6.append(loss_train)\n",
    "\n",
    "    y_pred_test = np.dot(X_test_standardized, w) + b\n",
    "    error_test = y_test - y_pred_test\n",
    "    loss_test = np.mean(error_test ** 2)\n",
    "    loss_history_test6.append(loss_test)\n",
    "\n",
    "    if i % 500 == 0 or i == (iterations-1):\n",
    "        print(f'iterations {i} : w = {np.round(w, 4)}, b = {b:.4f}, Train_Loss = {loss_train:.4f}, Test_Loss = {loss_test:.4f}')\n",
    "\n",
    "sns.set(style = 'whitegrid')\n",
    "plt.figure(figsize = (8, 6))\n",
    "plt.plot(loss_history_train6, \n",
    "         \n",
    "         label = 'Train Loss', color = 'blue')\n",
    "plt.plot(loss_history_test6, label = 'Test Loss', color = 'red', linestyle = 'dashed')\n",
    "plt.title('SGD + Momentum + Learning rate decay Loss vs Iterations', fontsize = 14)\n",
    "plt.xlabel('Iterations', fontsize = 12)\n",
    "plt.ylabel('Loss', fontsize = 12)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b9d76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "np.random.seed(42)\n",
    "\n",
    "w = np.random.rand(3)\n",
    "b = np.random.rand()\n",
    "\n",
    "initial_lr = 0.001\n",
    "iterations = 10000\n",
    "decay_rate = 0.0001\n",
    "lambda_reg = 0.001\n",
    "beta = 0.9\n",
    "\n",
    "# Cosine Annealing\n",
    "# Cosine Annealing with Warm Restarts (SGDR)\n",
    "eta_max = 0.001\n",
    "eta_min = 1e-6\n",
    "T_0 = 1000       # (初始週期長度 / SGDR)\n",
    "T_i = T_0        # (當前週期長度 / SGDR)\n",
    "T_start = 0      # (當前週期開始的 iteration / SGDR)\n",
    "\n",
    "# Step Decay\n",
    "drop_rate = 0.7\n",
    "step_size = 1000\n",
    "\n",
    "# Polynomial Decay\n",
    "power = 1\n",
    "\n",
    "v_w = np.zeros_like(w)\n",
    "v_b = 0\n",
    "\n",
    "loss_history_train7 = []\n",
    "loss_history_test7 = []\n",
    "\n",
    "lr_history = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    idx = np.random.randint(0, X_train_standardized.shape[0])\n",
    "    X_i = X_train_standardized[idx]\n",
    "    y_i = y_train[idx]\n",
    "\n",
    "    y_pred_i = np.dot(X_i, w) + b\n",
    "    error_i = y_i - y_pred_i\n",
    "\n",
    "    w_gradient = (-2) * X_i * error_i + 2 * lambda_reg * w\n",
    "    b_gradient = (-2) * error_i\n",
    "\n",
    "    # Decay 方法\n",
    "    # learning_rate = initial_lr * (1 - i/iterations) ** power   # Polynomial Decay (多項式學習率衰退)\n",
    "    # 當 power = 1 , 線性衰退(Linear Decay)\n",
    "    # 當 power = 2 , 學習率會更快降低，尾端更平滑\n",
    "    # 當 power < 1 ( 如 0.5 ) , 學習率初期下降慢、後期才加速下降\n",
    "\n",
    "    # learning_rate = initial_lr / (1 + decay_rate * i)  # Inverse Time Decay\n",
    "    # learning_rate = initial_lr * math.exp(-decay_rate * i)  # Exponential Decay\n",
    "    # learning_rate = eta_min + 0.5 * (eta_max - eta_min) * (1 + math.cos(math.pi * i / iterations))      # Cosine Annealing 學習率公式\n",
    "    # SGDR 更新週期 : 每當到達週期末時，就重設週期並倍增週期長度\n",
    "    if i - T_start >= T_i:\n",
    "        T_start = i\n",
    "        T_i *= 2             # 每次週期倍增\n",
    "    \n",
    "    T_cur = i - T_start      # 當前週期中的相對位置\n",
    "    learning_rate = eta_min + 0.5 * (eta_max - eta_min) * (1 + math.cos(math.pi * T_cur / T_i))\n",
    "\n",
    "    # learning_rate = initial_lr * (drop_rate ** (i // step_size))    # Step Decay\n",
    "\n",
    "    lr_history.append(learning_rate)\n",
    "\n",
    "    v_w = beta * v_w + (1 - beta) * w_gradient\n",
    "    v_b = beta * v_b + (1 - beta) * b_gradient\n",
    "\n",
    "    w -= learning_rate * v_w\n",
    "    b -= learning_rate * v_b\n",
    "\n",
    "    y_pred_train = np.dot(X_train_standardized, w) + b\n",
    "    error_train = y_train - y_pred_train\n",
    "\n",
    "    loss_train = np.mean(error_train ** 2) + lambda_reg * np.sum(w ** 2)\n",
    "    loss_history_train7.append(loss_train)\n",
    "\n",
    "    y_pred_test = np.dot(X_test_standardized, w) + b\n",
    "    error_test = y_test - y_pred_test\n",
    "    loss_test = np.mean(error_test ** 2)\n",
    "    loss_history_test7.append(loss_test)\n",
    "\n",
    "    if i % 50 == 0 or i == (iterations - 1):\n",
    "        print(f'iterations {i} : w = {np.round(w, 4)}, b = {b:.4f}, Train_Loss = {loss_train:.4f}, Test_Loss = {loss_test:.4f}')\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize = (10, 6))\n",
    "\n",
    "# 主軸 : 畫 Loss 曲線\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Iterations', fontsize = 12)\n",
    "ax1.set_ylabel('Loss', color = color , fontsize = 12)\n",
    "ax1.plot(loss_history_train7, label = 'Train Loss', color = 'blue')\n",
    "ax1.plot(loss_history_test7, label = 'Test Loss', color = 'red', linestyle = 'dashed')\n",
    "ax1.tick_params(axis = 'y', labelcolor = color)\n",
    "ax1.legend(loc = 'upper right')\n",
    "ax1.set_title('SGD + Momentum + SGDR: Loss vs Learning Rate', fontsize = 14)\n",
    "\n",
    "# 副軸 : 畫 Learning Rate 曲線\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:green'\n",
    "ax2.set_ylabel('Learning Rate', color = color, fontsize = 12)\n",
    "ax2.plot(lr_history, label = 'Learning Rate', color = color, linestyle = 'dotted')\n",
    "ax2.tick_params(axis = 'y', labelcolor = color)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
